id,abstract
1,"he field of machine learning has proven to be a powerful approach in smart manufacturing and processing in the chemical and process industries. This review provides a systematic overview of current state of artificial intelligence and machine learning and their applications in textile, nuclear power plant, fertilizer, water treatment, and oil and gas industries. Moreover, this study reveals the current dominant machine learning methods, pre and post processing of models, increased utilization of machine learning in terms of fault detection, prediction, optimization, quality control, and maintenance in these sectors. In addition, this review gives the insight into the actual benefits and impact of each method, and complications in their extensive deployment. Finally in the current impressive state, challenges, future development in terms of algorithm and infrastructure aspects are highlighted."
2,"Volunteer cotton (VC) plants growing in the fields of inter-seasonal and rotated crops, like corn, can serve as hosts to boll weevil pests once they reach pin-head square stage (5–6 leaf stage). The VC plants therefore need to be detected, located, and destroyed or sprayed. In this paper, we present a study on using deep learning (DL) to detect VC plants in a corn field using RGB images collected with an unmanned aerial vehicle (UAV). The objectives were (i) to determine whether the YOLOv3 DL algorithm could be used for VC detection in a corn field based on UAV-derived RGB images, and (ii) to investigate the behavior of YOLOv3 on images at three different pixel scales (320 × 320, S1; 416 × 416, S2; and 512 × 512, S3). The metrics used to evaluate the results were average precision (AP), mean average precision (mAP) and F1-score at 95 % confidence level. It was found that YOLOv3 was able to detect VC plants in corn field at an average detection accuracy of more than 80 %, F1-score of 78.5 % and mAP of 80.38 %. With respect to images size, no significant differences existed for mAP among the three scales, but a significant difference was found for AP between S1 and S3 (p = 0.04) and between S2 and S3 (p = 0.02). A significant difference was also found for F1-score between S2 and S3 (p = 0.02). The overall goal of this study was to minimize boll weevil pest infestation by maximizing the true positive detection of VC plants in a corn field which is represented by the mAP values. The lack of significant differences of these at all three scales indicated that the trained YOLOv3 model can be used for VC detection irrespective of the three input image sizes. The capability of YOLOv3 to detect VC plants demonstrates the potential of DL algorithms for real-time detection and mitigation using computer vision and a spot-spray capable UAV."
3,"Biomedical image analysis methods are gradually shifting towards computer-aided solutions from manual investigations to save time and improve the quality of the diagnosis. Deep learning-assisted biomedical image analysis is one of the major and active research areas. Several researchers are working in this domain because deep learning-assisted computer-aided diagnostic solutions are well known for their efficiency. In this chapter, a comprehensive overview of the deep learning-assisted biomedical image analysis methods is presented. This chapter can be helpful for the researchers to understand the recent developments and drawbacks of the present systems. The discussion is made from the perspective of the computer vision, pattern recognition, and artificial intelligence. This chapter can help to get future research directions to exploit the blessings of deep learning techniques for biomedical image analysis."
4,"Background In recent years, machine learning (ML) has had notable success in providing automated analyses of neuroimaging studies, and its role is likely to increase in the future. Thus, it is paramount for clinicians to understand these approaches, gain facility with interpreting ML results, and learn how to assess algorithm performance."
5,"Due to the digitalization of life and the fiercely competitive global market, the fourth industrial revolution was inevitable. Industry 4.0 utilizes several interconnected technologies such as artificial intelligence (AI), machine learning (ML), big data (BD) to provide new solutions. The aim of this article is to provide an overview of the vital role that these technologies play in the realization and adoption of Industry 4.0, the numerous merits they can yield, and the multitude of contemporary solutions, applications, and services they can provide. Therefore, this article presents the concept of Industry 4.0 as well as those of AI, ML, BD, and big data analytics (BDA) technologies. Moreover, it goes over the potentials that these technologies could offer and the merits they could yield when applied within the context of Industry 4.0. Finally, it presents the summary of the main findings, open research issues and challenges, draws conclusions, and provides directions for future research."
6,"Transformer architectures are highly expressive because they use self-attention mechanisms to encode long-range dependencies in the input sequences. In this paper, we present a literature review on Transformer-based (TB) models, providing a detailed overview of each model in comparison to the Transformer’s standard architecture. This survey focuses on TB models used in the field of Natural Language Processing (NLP) for textual-based tasks. We begin with an overview of the fundamental concepts at the heart of the success of these models. Then, we classify them based on their architecture and training mode. We compare the advantages and disadvantages of popular techniques in terms of architectural design and experimental value. Finally, we discuss open research, directions, and potential future work to help solve current TB application challenges in NLP."
7,"To stop climate change caused by the anthropogenic greenhouse effect, the amount of research project funding to develop and demonstrate solutions for carbon emission reduction has increased. Focused topics and procedures of research projects are always presented in text-based descriptions. Natural language processing offers the possibility to process and analyze data like this. In this work, we used natural language processing to extract keywords from descriptions of energy research projects with the algorithms TextRank and TF-IDF which has not been done for this kind of data basis before. A survey-based validation showed TF-IDF to be better suited for our data basis. Extracted keywords were used to conduct a keyword network analysis and calculate static and dynamic indices of the words concerning their recent importance and development over the past two decades. Further insights are shown by allocating the research projects’ amounts of funding to the extracted keywords. We found energy research to be more focused on individual components in the past. The last years were characterized by projects on energy systems and the interaction of renewable energy technologies and their integration into existing infrastructure. Finally, the results were compared with governmental research programs and we could analyze a comprehensive agreement."
8,"This paper reviews the current state of ChatGPT technology in finance and its potential to improve existing NLP-based financial applications. We discuss the ethical and regulatory considerations, as well as potential future research directions in the field. The literature suggests that ChatGPT has the potential to improve NLP-based financial applications, but also raises ethical and regulatory concerns that need to be addressed. The paper highlights the need for research in robustness, interpretability, and ethical considerations to ensure responsible use of ChatGPT technology in finance."
9,"With advancements in sequencing and proteomics approaches, computational functional annotation of proteins is becoming increasingly crucial. Among these annotations, prediction of post-translational modification (PTM) sites and prediction of function given a protein sequence are two very important problems. Recently, there have been several breakthroughs in Natural Language Processing (NLP) area. Consequently, we have observed an increase in the application of NLP-based techniques in the field of protein bioinformatics. In this chapter, we review various NLP-based encoding techniques for representation of protein sequences. Especially, we classify these approaches based on local/sparse encodings, distributed representation encodings, context-independent word embeddings, contextual word embedding and recent language models based pre-trained encodings. We summarize some of the recent approaches that make use of these NLP-based encodings for the prediction of various types of protein PTM sites and protein functions based on Gene Ontology (GO). Finally, we provide an outlook on possible future research directions for the NLP-based approaches for PTM sites and protein function predictions."
10,"Protein structure and function prediction are instrumental areas in the bioinformatics field. They are important for a number of applications in rational drug discovery, disease analysis, and many others. Protein sequences and natural languages share some similarities. Therefore, many techniques derived from natural language processing (NLP) have been applied to the protein structure and function prediction. In this chapter, we discuss sequence-based predictors of protein structure and function that utilize techniques derived from NLP field. We include methods that target protein sequence analysis, fold recognition, identification of intrinsically disordered regions/proteins, and prediction of protein-nucleic acids binding. The concepts and computational methods discussed in this chapter will be especially useful for the researchers who are working in the related field. We also aim to bring new computational NLP techniques into the protein structure and function prediction area."
11," Cloud computing security is a mushrooming service that has identical features to conventional IT security. It offers protecting sensitive information from data theft, loss and destruction. One of the advantages of cloud services is that you can work at scale.  But is the system secured properly? The system is vulnerable to attacks from many directions e.g., unsecure network, data losses, data breaches, cyber-attacks, reduced control etc. Therefore, we have new opportunities to deliver security solutions that meet new challenges. This paper discusses on these issues and provide remedial suggestions which can be focused on improving the system.  "
12,"The fast growth of the cloud computing technology has led to immense development in the public and private sectors. Cloud computing provides a high level of virtualization, massive scalability, multitenancy and elasticity. This has enabled organizations, academia, government departments and the public to move forward with this technology. However, they are unable to assuredly place their information over the clouds due to many security threats. Cloud security plays a vital role to establish a confidence between the cloud service providers, consumers and the multi-users to maintain the security levels of their data. This paper focuses the survey for cloud security issues, existing authentication schemes, data storage technologies and offers a glimpse of Artificial Neural Networks (ANNs) applied to the cloud security."
13,"The early identification of COVID-19 is critical to prevent the disease from spreading at the community level. Cloud computing allows healthcare providers to enhance patient care, exchange information more quickly, increase operational efficiency, and save expenses. Thus, this research investigates cloud computing applications in the context of an outbreak. We gathered, summarized, and evaluated scientific papers on cloud computing throughout an outbreak that were released between 2020 and April 2022 for this Systematic Literature Review (SLR). We employed a pre-defined review methodology to examine commonly known electronic datasets. Keywords were used to look for all publications connected to the subject. 18 papers were chosen for this SLR after thoroughly following the research selection method. This review paper offers current state-of-the-art outcomes and methodologies on cloud computing throughout an epidemic, outlining research gaps and providing directions for future research. The results showed that cloud computing plays a crucial role in addressing and relieving side effects in vital situations such as the coronavirus epidemic. We tried to provide a comprehensive study, but non-English articles were not included in this study, so future studies can provide a more comprehensive study by considering all articles in Chinese, Japanese, etc."
14,"Cloud Computing (CC) has emerged as one of the most discussed topics among enterprise information technology (IT) professionals. Small and Medium Enterprises (SMEs) with low budget and human resources are one of the major groups that tend to use CC for achieving the benefit of this technology. A multitude of factors influence the adoption of CC for SMEs. These decisive factors must be systematically evaluated prior to making the decision to adopt cloud-based solutions. Integration of supply chain activities and the technologies to accomplish it have become competitive necessities in most industries. Accordingly, the trend toward greater use of supply chain technologies is on a clear path forward. As one manager has noted: ‘‘With almost daily technology advancement globally in every facet of the business, organizations need to synchronize by adopting and implementing new electronic commerce and supply chain technology in order to protect market share, not to mention improve market penetration’’. This paper develops a model of the key factors influencing the adoption of supply chain based on cloud computing technology. The following set of variables were hypothesized to have a significant impact upon the pace of technology adoption: relative advantage, compatibility, security concerns, cost savings, technology readiness, top manager support, competitive pressure and regulatory support. The model provides a better understanding of the supply chain technology diffusion process. The purpose of this study is to identify these factors and determine the extent to which they influence the adoption of CC for SMEs. Therefore, the project describes a research model that is based on the diffusion of innovation (DOI) theory and the technology, organization and environment (TOE) framework. Data was collected by survey questionnaires from a sample of 22 SMEs that all of these SMEs as a customer’s of one cloud provider. There are 77 experts in information technology department from those SMEs are selected to fill the questionnaires. The Smart PLS tool was used for data analysis. The results of the data analysis generally support the model, as well as all of the proposed hypotheses. In summary, the results of this research have shown relative advantage, compatibility, security concerns, cost savings, technology readiness, top manager support, competitive pressure and regulatory support were found to have significant influence on adoption of supply chain management based on CC for SMEs."
15,"With the fast improvement of Internet social platforms, consumer evaluations have emerged as a critical foundation for clients to recognize merchandise and make decisions. New means of communication, such as microblogging, have evolved in the last decade, and a lot of information is provided by tweets and short messages, that are used to predict feelings of the users and their perception of what is going on around the globe. The most prominent sentiment in the tweet can be recognized by sentiment analysis techniques. Sentiment analysis is the method of extracting and recognizing the user’s evaluations of products and models and has various approaches using machine learning algorithms to classify the emotion behind that text. This paper investigates the usage of Enhanced BERT models to recognize the sentiment behind the tweet. BERT or bi-directional encoder representations from transformers was designed to help computers understand the meaning of ambiguous language in text, by using the surrounding text to understand the context in which that text could have been written. For a successful evaluation using Enhanced BERT, the Kaggle SMILE dataset is considered and will be tested for emotions such as happiness, sadness, etc., and categorized as such. Experiments display that this version of the model achieves an accuracy of 0.96."
16,"Evaluation of students’ feedback is essential in education as it helps the instructors to check the effectiveness of their teaching. The feedback collected at the end of the semester comprises questionnaires and open-ended questions. It is very difficult to manually analyze comments given by the students in response to open-ended questions. This paper proposes a method to extract opinions from students’ feedback that will help to improve the teaching–learning process. It deals with different aspects of teaching i.e. punctuality, the pace of the teaching, subject knowledge, etc. It also incorporates a hybrid approach that combines the lexicon and machine learning approaches. In the lexicon approach, various linguistics features i.e. negation, contact shifters, and modifiers have been considered as these change or add to the orientation of the sentence. SentiWordNet dictionary has been used to assign score to words in the sentences and based on the score, the sentence has been classified as positive, negative, or neutral. After assigning the orientation, the dataset has been resampled using various resampling techniques i.e. ENN, TL, OSS, NCR, SMOTE, ADASYN, Borderline SMOTE, SMOTE-ENN, SMOTE-Tomek, etc. These techniques have been applied to balance the class distribution of each aspect. Then, machine learning algorithms i.e. SVM, MNB, LR, RFC, DTC, and KNN have been applied to the dataset. Experimental results indicate that the proposed approach outperforms other baseline methods in the context of automated sentiment scoring and has achieved 98.7% aggregate accuracy using the RFC algorithm on the students’ feedback dataset."
17,"This study will discuss customers’ satisfaction with the services of Traveloka by analyzing how many people are satisfied and unhappy with the services that Traveloka has to offer. This study uses Twitter to acquire all the data we need, focusing only on tweets about Traveloka. The dataset is gathered from Twitter API, which consists of 1200 tweets related to Traveloka. Scikit-learn library is used through python to do the analysis process. This research employs three classification metheods: Support Vector Model (SVM), Logistic Regression, and Na¨ıve Bayes. The steps in this research were data retrieval, transformation, classification training and predicting the test data, and finally, the result analysis. Therefore, this research is looking forward to how most Twitter users feel about the performance of this mobile traveling application. The result shows that SVM has better accuracy in determining the sentiment of tweets about Traveloka."
18,"Scholarly work regarding tourist behavior requires more empirical support especially concerning methods to extract emotions and sentiments from user-generated comments on social media. Research on predicting behavioral intentions of foreign tourists who faced culture shock (or unexpected surprise) has been under-represented in tourism literature. This paper used Aspect Based Sentiment Analysis (ABSA) and Emotion analysis (EA) to predict tourist behavior from User Generated Comments (UGCs) post their travel in three Asian countries. Findings reveal the presence of a sizeable inactive group of tourists who were indifferent towards post-travel behavioral intentions. The study recommends destination managers to consider managing tourist emotions, specifically sad surprise, in the travel phase of the tourists to strategically manage the post-travel behavior of tourists on social media. A conceptual model has been presented to operationalise these constructs to broadly predict destination optimism and destination pessimism proclivities of the tourists."
19,"Multimodal sentiment analysis has attracted increasing attention with broad application prospects. Most of the existing methods have focused on a single modality, which fails to handle social media data due to its multiple modalities. Moreover, in multimodal learning, most of the works have focused on simply combining the two modalities without exploring the complicated correlations between them. This resulted in dissatisfying performance for multimodal sentiment classification. Motivated by the status quo, we propose a Deep Multi-level Attentive network (DMLANet), which exploits the correlation between image and text modalities to improve multimodal learning. Specifically, we generate the bi-attentive visual map along the spatial and channel dimensions to magnify Convolutional neural network representation power. Then, we model the correlation between the image regions and semantics of the word by extracting the textual features related to the bi-attentive visual features by applying semantic attention. Finally, self-attention is employed to fetch the sentiment-rich multimodal features for the classification automatically. We conduct extensive evaluations on four real-world datasets, namely, MVSA-Single, MVSA-Multiple, Flickr, and Getty Images, which verify our method's superiority."
20,"Understanding the relationship between biodiversity conservation and ecosystem services concepts is essential for evidence-based policy development. We used text mining augmented by topic modelling to analyse abstracts of 15 310 peer-reviewed papers (from 2000 to 2020). We identified nine major topics; “Research & Policy”, “Urban and Spatial Planning”, “Economics & Conservation”, “Diversity & Plants”, “Species & Climate change”, “Agriculture”, “Conservation and Distribution”, “Carbon & Soil & Forestry”, “Hydro-& Microbiology”. The topic “Research & Policy” performed highly, considering number of publications and citation rate, while in the case of other topics, the “best” performances varied, depending on the indicator applied. Topics with human, policy or economic dimensions had higher performances than the ones with ‘pure’ biodiversity and science. Agriculture dominated over forestry and fishery sectors, while some elements of biodiversity and ecosystem services were under-represented. Text mining is a powerful tool to identify relations between research supply and policy demand."
21,"The outbreak of the COVID-19 pandemic has significantly increased the demand for personal protective equipment, in particular face masks, thus leading to a huge amount of healthcare waste generated worldwide. Consequently, such an unprecedented amount of newly emerged waste has posed significant challenges to practitioners, policy-makers, and municipal authorities involved in waste management (WM) systems. This research aims at mapping the COVID-19-related scientific production to date in the field of WM. In this vein, the performance indicators of the target literature were analyzed and discussed through conducting a bibliometric analysis. The conceptual structure of COVID-19-related WM research, including seven main research themes, were uncovered and visualized through a text mining analysis as follows: (1) household and food waste, (2) personnel safety and training for waste handling, (3) sustainability and circular economy, (4) personal protective equipment and plastic waste, (5) healthcare waste management practices, (6) wastewater management, and (7) COVID-19 transmission through infectious waste. Finally, a research agenda for WM practices and activities in the post-COVID-19 era was proposed, focusing on the following three identified research gaps: (i) developing a systemic framework to properly manage the pandemic crisis implications for WM practices as a whole, following a systems thinking approach, (ii) building a circular economy model encompassing all activities from the design stage to the implementation stage, and (iii) proposing incentives to effectively involve informal sectors and local capacity in decentralizing municipal waste management, with a specific focus on developing and less-developed countries."
22,"While text mining and NLP research has been established for decades, there remain gaps in the literature that reports the use of these techniques in building real-world applications. For example, they typically look at single and sometimes simplified tasks, and do not discuss in-depth data heterogeneity and inconsistency that is common in real-world problems or their implication on the development of their methods. Also, few prior work has focused on the healthcare domain. In this work, we describe an industry project that developed text mining and NLP solutions to mine millions of heterogeneous, multilingual procurement documents in the healthcare sector. We extract structured procurement contract data that is used to power a platform for dynamically assessing supplier risks. Our work makes unique contributions in a number of ways. First, we deal with highly heterogeneous, multilingual data and we document our approach to tackle these challenges. This is mainly based on a method that effectively uses domain knowledge and generalises to multiple text mining and NLP tasks and languages. Second, applying this method to mine millions of procurement documents, we develop the first structured procurement contract database that will help facilitate the tendering process. Second, Finally, we discuss lessons learned for practical text mining/NLP development, and make recommendations for future research and practice."
23,"The dramatic increase in the number of microbe descriptions in databases, reports, and papers presents a two-fold challenge for accessing the information: integration of heterogeneous data in a standard ontology-based representation and normalization of the textual descriptions by semantic analysis. Recent text mining methods offer powerful ways to extract textual information and generate ontology-based representation. This paper describes the design of the Omnicrobe application that gathers comprehensive information on habitats, phenotypes, and usages of microbes from scientific sources of high interest to the microbiology community. The Omnicrobe database contains around 1 million descriptions of microbe properties. These descriptions are created by analyzing and combining six information sources of various kinds, i.e. biological resource catalogs, sequence databases and scientific literature. The microbe properties are indexed by the Ontobiotope ontology and their taxa are indexed by an extended version of the taxonomy maintained by the National Center for Biotechnology Information. The Omnicrobe application covers all domains of microbiology. With simple or rich ontology-based queries, it provides easy-to-use support in the resolution of scientific questions related to the habitats, phenotypes, and uses of microbes. We illustrate the potential of Omnicrobe with a use case from the food innovation domain."
24,"In the field of text classification, news text classification has always been the focus of research, and it is quite difficult. With the gradual maturity of deep learning technology and the advent of the information explosion era, the traditional text classification method has been unable to meet people's needs for rapid, efficient, and accurate news classification. The neural network based on bidirectional encoder representations from transformers (BERT) in deep learning is very suitable for news text classification. However, the random mask masking method adopted by BERT model does not incorporate the knowledge information of Chinese language, resulting in the problem of low classification accuracy. Based on this problem, this paper proposes a news topic classification method based on the ERNIE model of multi-stage fusion knowledge masking strategy. When the Chinese language knowledge information fusion model deals with the Chinese news text classification problem, the BERT model, BERT-CNN model, and ConvBERT model are compared. The results show that ERNIE model is superior to other models in the task of news text classification."
25,"This study investigates the possibilities offered by the development of graphene environment technology that can contribute to new and effective approaches for the transition to an ecologically benign ecosystem. To analyze the research and development progress in graphene environment technology, graphene environment technology research trends of South Korea for the years 2009–2020 are investigated by acquiring information pertaining to national research and development projects related to graphene environment technology from the National Science and Technology Information Service. Both structured and unstructured data are analyzed using diverse text-mining methods, such as keyword frequency analysis, association rule mining, and topic modelling. The results indicate that graphene research in South Korea is focused primarily on graphene use in batteries and energy-storage devices, such as solar cells, fuel cells, and secondary batteries. This study can help understand the manner by which the South Korean government has been investing in the research and development of graphene environment technology; additionally, it discusses the future applications and prospects of graphene for the next decade."
26,"In this paper, we propose DeepSumm, a novel method based on topic modeling and word embeddings for the extractive summarization of single documents. Recent summarization methods based on sequence networks fail to capture the long range semantics of the document which are encapsulated in the topic vectors of the document. In DeepSumm, our aim is to utilize the latent information in the document estimated via topic vectors and sequence networks to improve the quality and accuracy of the summarized text. Each sentence is encoded through two different recurrent neural networks based on probabilistic topic distributions and word embeddings, and then a sequence to sequence network is applied to each sentence encoding. The outputs of the encoder and the decoder in the sequence to sequence networks are combined after weighting using an attention mechanism and converted into a score through a multi-layer perceptron network. We refer to the score obtained through the topic model as Sentence Topic Score (STS) and to the score generated through word embeddings as Sentence Content Score (SCS). In addition, we propose Sentence Novelty Score (SNS) and Sentence Position Score (SPS) and perform a weighted fusion of the four scores for each sentence in the document to compute a Final Sentence Score (FSS). The proposed DeepSumm framework was evaluated on the standard DUC 2002 benchmark and CNN/DailyMail datasets. Experimentally, it was demonstrated that our method captures both the global and the local semantic information of the document and essentially outperforms existing state-of-the-art approaches for extractive text summarization with ROUGE-1, ROUGE-2, and ROUGE-L scores of 53.2, 28.7 and 49.2 on DUC 2002 and 43.3, 19.0 and 38.9 on CNN/DailyMail dataset."
27,"With recent advancements in the area of Natural Language Processing, the focus is slowly shifting from a purely English-centric view towards more language-specific solutions, including German. Especially practical for businesses to analyze their growing amount of textual data are text summarization systems, which transform long input documents into compressed and more digestible summary texts. In this work, we assess the particular landscape of German abstractive text summarization and investigate the reasons why practically useful solutions for abstractive text summarization are still absent in industry. Our focus is two-fold, analyzing a) training resources, and b) publicly available summarization systems. We are able to show that popular existing datasets exhibit crucial flaws in their assumptions about the original sources, which frequently leads to detrimental effects on system generalization and evaluation biases. We confirm that for the most popular training dataset, MLSUM, over 50% of the training set is unsuitable for abstractive summarization purposes. Furthermore, available systems frequently fail to compare to simple baselines, and ignore more effective and efficient extractive summarization approaches. We attribute poor evaluation quality to a variety of different factors, which are investigated in more detail in this work: A lack of qualitative (and diverse) gold data considered for training, understudied (and untreated) positional biases in some of the existing datasets, and the lack of easily accessible and streamlined pre-processing strategies or analysis tools. We provide a comprehensive assessment of available models on the cleaned datasets, and find that this can lead to a reduction of more than 20 ROUGE-1 points during evaluation."
28,"Due to the exponential increase in the generation of digital documents and in the online search user diversity, multilingual information is highly available on the Internet. However, the huge amount of multilingual data cannot be analyzed manually. Therefore, cross lingual multi-document summarization (CLMDS) model is introduced to generate a summary of several documents in which the summary language is different from the source document language. This paper presents a Deep Learning Enabled Cross-lingual Search with Metaheuristic based Query Optimization (DLCLS-MQO) model for Multi-document summarization. The DLCLS-MQO model allows to offer a query in Tamil, summarize several English documents, and lastly translate the summary into Tamil. The DLCLS-MQO model encompasses four stages of operation such as multilingual search, query optimization, automatic sematic lexicon builder, and document summarization. Firstly, bidirectional long short-term memory (BiLSTM) model is applied to perform multilingual searching process. Followed by, sunflower optimization (SFO) algorithm based query optimization process is carried out. Moreover, global vectors (GloVe) method is used for the construction of domain oriented sentiment lexicons. Finally, extreme gradient boosting (XGBoost) model is applied for the CLMDS. A detailed simulation analysis takes place to highlight the betterment of the DLCLS-MQO model. The resultant experimental values portrayed the superior performance of the DLCLS-MQO model over the compared methods."
29,"Text summarization on non-factoid question answering (NQA) aims at identifying the core information of redundant answer guidance using questions, which can dramatically improve answer readability and comprehensibility. Most existing approaches focus on extracting query-related sentences to construct a summary, where the logical connection of natural language and the hierarchical interpretable semantic association are often neglected, thus degrading performance. To address these issues, we propose a novel question-driven abstractive answer summarization model, called the Hierarchical Sliding Inference Generator (HSIG), to form inferable and interpretable summaries by explicitly introducing hierarchical information reasoning between questions and corresponding answers. Specifically, we first apply an elaborately designed hierarchical sliding fusion inference model to determine the most relevant question sentence-level representation that provides a deeper interpretable basis for sentence selection in summarization, which further increases computational performance on the premise of following the semantic inheritance structure. Additionally, to improve summary fluency, we construct a double-driven selective generator to integrate various semantic information from two mutual question-and-answer perspectives. Experimental results illustrate that compared with state-of-the-art baselines, our model achieves remarkable improvement on two benchmark datasets and specifically improves the 2.46 ROUGE-1 points on PubMedQA, which demonstrates the superiority of our model on abstractive summarization with hierarchical sequential reasoning."
30,"Abstractive Text Summarization (ATS), which is the task of constructing summary sentences by merging facts from different source sentences and condensing them into a shorter representation while preserving information content and overall meaning. It is very difficult and time consuming for human beings to manually summarize large documents of text. In this paper, we propose an LSTM-CNN based ATS framework (ATSDL) that can construct new sentences by exploring more fine-grained fragments than sentences, namely, semantic phrases. Different from existing abstraction based approaches, ATSDL is composed of two main stages, the first of which extracts phrases from source sentences and the second generates text summaries using deep learning. Experimental results on the datasets CNN and DailyMail show that our ATSDL framework outperforms the state-of-the-art models in terms of both semantics and syntactic structure, and achieves competitive results on manual linguistic quality evaluation."
