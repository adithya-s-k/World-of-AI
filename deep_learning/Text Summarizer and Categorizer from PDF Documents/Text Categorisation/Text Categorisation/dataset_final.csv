Title,Abstract
Natural Language Processing,"The abundant volume of natural language text in the connected world, though having a large content of knowledge, but it is becoming increasingly difficult to disseminate it by a human to discover the knowledge/wisdom in it, specifically within any given time limits. The automated NLP is aimed to do this job effectively and with accuracy, like a human does it (for a limited of amount text). This chapter presents the challenges of NLP, progress so far made in this field, NLP applications, components of NLP, and grammar of English language—the way machine requires it. In addition, covers the specific areas like probabilistic parsing, ambiguities and their resolution, information extraction, discourse analysis, NL question-answering, commonsense interfaces, commonsense thinking and reasoning, causal-diversity, and various tools for NLP. Finally, the chapter summary, and a set of relevant exercises are presented. Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers."
"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing","This article surveys and organizes research works in a new paradigm in natural language processing, which we dub “prompt-based learning.” Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x′ that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x̂, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: It allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this article, we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g., the choice of pre-trained language models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts but also release other resources, e.g., a website NLPedia–Pretrain including constantly updated survey and paperlist."
Graph Neural Networks for Natural Language Processing: A Survey,"Deep learning has become the dominant approach in addressing various tasks in Natural Language Processing (NLP). Although text inputs are typically represented as a sequence of tokens, there is a rich variety of NLP problems that can be best expressed with a graph structure. As a result, there is a surge of interest in developing new deep learning techniques on graphs for a large number of NLP tasks. In this survey, we present a comprehensive overview on Graph Neural Networks (GNNs) for Natural Language Processing. We propose a new taxonomy of GNNs for NLP, which systematically organizes existing research of GNNs for NLP along three axes: graph construction, graph representation learning, and graph based encoder-decoder models. We further introduce a large number of NLP applications that exploits the power of GNNs and summarize the corresponding benchmark datasets, evaluation metrics, and open-source codes. Finally, we discuss various outstanding challenges for making the full use of GNNs for NLP as well as future research directions. To the best of our knowledge, this is the first comprehensive overview of Graph Neural Networks for Natural Language Processing."
I just want to matter: Examining the role of anti-mattering in online suicide support communities using natural language processing,"Annually, tens of thousands of lives are affected by suicide, including those who die by suicide, attempt non-fatal suicidal behaviors, and suffer from suicidal thoughts and ideation as well as friends and family affected by the loss of loved ones. Although a number of predictors of suicidal thoughts and behaviors have been examined in the literature, the current study investigated the presence of anti-mattering and suicidal ideation in online peer support communities through the r/SuicideWatch subreddit community. A historical archive of posts made in the subreddit was collected using the Python Reddit API Wrapper (PRAW), resulting in a final anonymized corpus of 500,548 posts. A sample of 1,700 posts were hand-labeled for the presence of anti-mattering and other constructs in order to train and evaluate a classification model for automated labeling. Using the resulting model and keyword dictionaries, the entire corpus was labeled, informing a nomological network of anti-mattering in online peer-support communities."
Data augmentation in natural language processing: a novel text generation approach for long and short text classifiers,"In many cases of machine learning, research suggests that the development of training data might have a higher relevance than the choice and modelling of classifiers themselves. Thus, data augmentation methods have been developed to improve classifiers by artificially created training data. In NLP, there is the challenge of establishing universal rules for text transformations which provide new linguistic patterns. In this paper, we present and evaluate a text generation method suitable to increase the performance of classifiers for long and short texts. We achieved promising improvements when evaluating short as well as long text tasks with the enhancement by our text generation method. Especially with regard to small data analytics, additive accuracy gains of up to 15.53% and 3.56% are achieved within a constructed low data regime, compared to the no augmentation baseline and another data augmentation technique. As the current track of these constructed regimes is not universally applicable, we also show major improvements in several real world low data tasks (up to +4.84 F1-score). Since we are evaluating the method from many perspectives (in total 11 datasets), we also observe situations where the method might not be suitable. We discuss implications and patterns for the successful application of our approach on different types of datasets."
Data augmentation techniques in natural language processing,"Data Augmentation (DA) methods – a family of techniques designed for synthetic generation of training data – have shown remarkable results in various Deep Learning and Machine Learning tasks. Despite its widespread and successful adoption within the computer vision community, DA techniques designed for natural language processing (NLP) tasks have exhibited much slower advances and limited success in achieving performance gains. As a consequence, with the exception of applications of back-translation to machine translation tasks, these techniques have not been as thoroughly explored by the wider NLP community. Recent research on the subject still lacks a proper practical understanding of the relationship between the various existing DA methods. The connection between DA methods and several important aspects of its outputs, such as lexical diversity and semantic fidelity, is also still poorly understood. In this work, we perform a comprehensive study of NLP DA techniques, comparing their relative performance under different settings. We analyze the quality of the synthetic data generated, evaluate its performance gains and compare all of these aspects to previous existing DA procedures."
An End-to-End Natural Language Processing Application for Prediction of Medical Case Coding Complexity: Algorithm Development and Validation,"Medical coding is the process that converts clinical documentation into standard medical codes. Codes are used for several key purposes in a hospital (eg, insurance reimbursement and performance analysis); therefore, their optimization is crucial. With the rapid growth of natural language processing technologies, several solutions based on artificial intelligence have been proposed to aid in medical coding by automatically suggesting relevant codes for clinical documents. However, their effectiveness is still limited to simple cases, and it is not yet clear how much value they can bring in improving coding efficiency and accuracy."
Implementing associative memories by Echo State Network for the applications of natural language processing,"This paper presents Echo State Network (ESN) based associative memories and their applications to English words. Among the papers describing ESN applications in general, some papers deal with natural language processing (NLP). Those NLP papers utilize an ESN’s property of temporal signal learning to translate an English sentence into its corresponding predicate logic formula. In practical NLP applications, input sentences often include misspelled or undefined words. To cope with such problems, we constructed a training algorithm to realize an ESN-based associative memory. This algorithm can be used in two ways: auto-associative one when the input and output patterns are the same in each pair of the training data, and hetero-associative one otherwise. In this paper, we firstly show the basic performance of an ESN-based auto-associative memory by applying it to two-dimensional images. We made sure that the performance is improved by averaging the result of multiple runs. Secondly, we describe the performances of ESN-based auto- and hetero-associative memories when being applied to English words. The former is to recall correct English words from incomplete spelling ones, and the latter is to chain two different English words of an input and its corresponding output. The former performance in recalling 26 incomplete words can be improved by averaging the outputs of multiple runs, and the improved success rate of correct words is around 65%. The latter performance in chaining 26 words attains the success rate of 83.4%."
Discovering a Failure Taxonomy for Early Design of Complex Engineered Systems Using Natural Language Processing ,"Methodologies for failure assessment frequently rely on historical failure modes, causes, and recommendations for prevention. Meanwhile, there are growing databases of narrative-based lessons that are under-utilized due to their size. Advances in natural language processing (NLP) enable unsupervised extraction of this knowledge. We present a methodology for (1) identifying relevant information using a term frequency inverse document frequency (TF-IDF) classifier and (2) extracting knowledge for failure assessment using a hierarchical topic modeling approach, hierarchical latent Dirichlet allocation (LDA). To interpret the extracted topics, we apply an automatic topic labeling technique using pointwise mutual information (PMI) extraction. The methodology is applied to NASA’s Lessons Learned Information System (LLIS), which is publicly available. Partitioned topics enable the extraction of three aspects: cause, failure, and recommendation, while a hierarchy enables organization into a taxonomy. The methodology is generalizable to databases containing narrative-style documents, while the results from the LLIS represent a summary of themes in the dataset, expressed in a format that can be linked to early design failure analyses."
How Do Users Respond to Mass Vaccination Centers? A Cross-Sectional Study Using Natural Language Processing on Online Reviews to Explore User Experience and Satisfaction with COVID-19 Vaccination Centers,"To reach large groups of vaccine recipients, several high-income countries introduced mass vaccination centers for COVID-19. Understanding user experiences of these novel structures can help optimize their design and increase patient satisfaction and vaccine uptake. This study drew on user online reviews of vaccination centers to assess user experience and identify its key determinants over time, by sentiment, and by interaction. Machine learning methods were used to analyze Google reviews of six COVID-19 mass vaccination centers in Berlin from December 2020 to December 2021. 3647 user online reviews were included in the analysis. Of these, 89% (3261/3647) were positive according to user rating (four to five of five stars). A total of 85% (2740/3647) of all reviews contained text. Topic modeling of the reviews containing text identified five optimally latent topics, and keyword extraction identified 47 salient keywords. The most important themes were organization, friendliness/responsiveness, and patient flow/wait time. Key interactions for users of vaccination centers included waiting, scheduling, transit, and the vaccination itself. Keywords connected to scheduling and efficiency, such as “appointment” and “wait”, were most prominent in negative reviews. Over time, the average rating score decreased from 4.7 to 4.1, and waiting and duration became more salient keywords. Overall, mass vaccination centers appear to be positively perceived, yet users became more critical over the one-year period of the pandemic vaccination campaign observed. The study shows that online reviews can provide real-time insights into newly set-up infrastructures, and policymakers should consider their use to monitor the population’s response over time."
Paradigm Shift in Natural Language Processing,"In the era of deep learning, modeling for most natural language processing (NLP) tasks has converged into several mainstream paradigms. For example, we usually adopt the sequence labeling paradigm to solve a bundle of tasks such as POS-tagging, named entity recognition (NER), and chunking, and adopt the classification paradigm to solve tasks like sentiment analysis. With the rapid progress of pre-trained language models, recent years have witnessed a rising trend of paradigm shift, which is solving one NLP task in a new paradigm by reformulating the task. The paradigm shift has achieved great success on many tasks and is becoming a promising way to improve model performance. Moreover, some of these paradigms have shown great potential to unify a large number of NLP tasks, making it possible to build a single model to handle diverse tasks. In this paper, we review such phenomenon of paradigm shifts in recent years, highlighting several paradigms that have the potential to solve different NLP tasks."
Natural language processing applied to mental illness detection: a narrative review,"Mental illness is highly prevalent nowadays, constituting a major cause of distress in people’s life with impact on society’s health and well-being. Mental illness is a complex multi-factorial disease associated with individual risk factors and a variety of socioeconomic, clinical associations. In order to capture these complex associations expressed in a wide variety of textual data, including social media posts, interviews, and clinical notes, natural language processing (NLP) methods demonstrate promising improvements to empower proactive mental healthcare and assist early diagnosis. We provide a narrative review of mental illness detection using NLP in the past decade, to understand methods, trends, challenges and future directions. A total of 399 studies from 10,467 records were included. The review reveals that there is an upward trend in mental illness detection NLP research. Deep learning methods receive more attention and perform better than traditional machine learning methods. We also provide some recommendations for future studies, including the development of novel detection methods, deep learning paradigms and interpretable models."
Natural language processing for smart construction: Current status and future directions,"Unstructured texts dominate data in construction projects. With the achievements of natural language processing (NLP) techniques, mining unstructured text data for smart construction has become increasingly significant. To understand state-of-the-art NLP for smart construction, uncover related issues, and propose potential improvements, this paper presents a comprehensive review of bottom-level techniques and mainstream applications of NLP in the industry. In total, 124 journal articles published in the last two decades are reviewed. NLP involves five core steps supported by various techniques, e.g., syntactic parsing, heuristic rules, machine learning, and deep learning. NLP has been applied for information extraction and exchanging and many downstream applications to facilitate management and decision-making. The role of NLP in smart construction and current challenges for fully reaping its benefits are discussed, and four research directions are identified, i.e., improving relation extraction, realising knowledge base auto-development, integrating multi-modal information, and achieving an accuracy-efficiency trade-off by developing an NLP application framework. It is envisioned that outcomes of this paper can assist both researchers and industrial practitioners with appreciating the research and practice frontier of NLP for smart construction and soliciting the latest NLP techniques."
Brains and algorithms partially converge in natural language processing,"Deep learning algorithms trained to predict masked words from large amount of text have recently been shown to generate activations similar to those of the human brain. However, what drives this similarity remains currently unknown. Here, we systematically compare a variety of deep language models to identify the computational principles that lead them to generate brain-like representations of sentences. Specifically, we analyze the brain responses to 400 isolated sentences in a large cohort of 102 subjects, each recorded for two hours with functional magnetic resonance imaging (fMRI) and magnetoencephalography (MEG). We then test where and when each of these algorithms maps onto the brain responses. Finally, we estimate how the architecture, training, and performance of these models independently account for the generation of brain-like representations. Our analyses reveal two main findings. First, the similarity between the algorithms and the brain primarily depends on their ability to predict words from context. Second, this similarity reveals the rise and maintenance of perceptual, lexical, and compositional representations within each cortical region. Overall, this study shows that modern language algorithms partially converge towards brain-like solutions, and thus delineates a promising path to unravel the foundations of natural language processing."
Data augmentation approaches in natural language processing: A survey,"As an effective strategy, data augmentation (DA) alleviates data scarcity scenarios where deep learning techniques may fail. It is widely applied in computer vision then introduced to natural language processing and achieves improvements in many tasks. One of the main focuses of the DA methods is to improve the diversity of training data, thereby helping the model to better generalize to unseen testing data. In this survey, we frame DA methods into three categories based on the diversity of augmented data, including paraphrasing, noising, and sampling. Our paper sets out to analyze DA methods in detail according to the above categories. Further, we also introduce their applications in NLP tasks as well as the challenges. Some useful resources are provided in Appendix A."
"An introduction to Deep Learning in Natural Language Processing: Models, techniques, and tools","Natural Language Processing (NLP) is a branch of artificial intelligence that involves the design and implementation of systems and algorithms able to interact through human language. Thanks to the recent advances of deep learning, NLP applications have received an unprecedented boost in performance. In this paper, we present a survey of the application of deep learning techniques in NLP, with a focus on the various tasks where deep learning is demonstrating stronger impact. Additionally, we explore, describe, and revise the main resources in NLP research, including software, hardware, and popular corpora. Finally, we emphasize the main limits of deep learning in NLP and current research directions."
"Causal Inference in Natural Language Processing: Estimation, Prediction, Interpretation and Beyond","A fundamental goal of scientific research is to learn about causal relationships. However, despite its critical role in the life and social sciences, causality has not had the same importance in Natural Language Processing (NLP), which has traditionally placed more emphasis on predictive tasks. This distinction is beginning to fade, with an emerging area of interdisciplinary research at the convergence of causal inference and language processing. Still, research on causality in NLP remains scattered across domains without unified definitions, benchmark datasets and clear articulations of the challenges and opportunities in the application of causal inference to the textual domain, with its unique properties. In this survey, we consolidate research across academic areas and situate it in the broader NLP landscape. We introduce the statistical challenge of estimating causal effects with text, encompassing settings where text is used as an outcome, treatment, or to address confounding. In addition, we explore potential uses of causal inference to improve the robustness, fairness, and interpretability of NLP models. We thus provide a unified overview of causal inference for the NLP community.1"
Pre-trained models for natural language processing: A survey,"Recently, the emergence of pre-trained models (PTMs) has brought natural language processing (NLP) to a new era. In this survey, we provide a comprehensive review of PTMs for NLP. We first briefly introduce language representation learning and its research progress. Then we systematically categorize existing PTMs based on a taxonomy from four different perspectives. Next, we describe how to adapt the knowledge of PTMs to downstream tasks. Finally, we outline some potential directions of PTMs for future research. This survey is purposed to be a hands-on guide for understanding, using, and developing PTMs for various NLP tasks."
Mitigating Gender Bias in Natural Language Processing: Literature Review,"As Natural Language Processing (NLP) and Machine Learning (ML) tools rise in popularity, it becomes increasingly vital to recognize the role they play in shaping societal biases and stereotypes. Although NLP models have shown success in modeling various applications, they propagate and may even amplify gender bias found in text corpora. While the study of bias in artificial intelligence is not new, methods to mitigate gender bias in NLP are relatively nascent. In this paper, we review contemporary studies on recognizing and mitigating gender bias in NLP. We discuss gender bias based on four forms of representation bias and analyze methods recognizing gender bias. Furthermore, we discuss the advantages and drawbacks of existing gender debiasing methods. Finally, we discuss future studies for recognizing and mitigating gender bias in NLP."
Transformers: State-of-the-Art Natural Language Processing,"Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers."
HuggingFace's Transformers: State-of-the-art Natural Language Processing,"Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \textit{Transformers} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \textit{Transformers} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. "
Attention in Natural Language Processing,"Attention is an increasingly popular mechanism used in a wide range of neural architectures. The mechanism itself has been realized in a variety of formats. However, because of the fast-paced advances in this domain, a systematic overview of attention is still missing. In this article, we define a unified model for attention architectures in natural language processing, with a focus on those designed to work with vector representations of the textual data. We propose a taxonomy of attention models according to four dimensions: the representation of the input, the compatibility function, the distribution function, and the multiplicity of the input and/or output. We present the examples of how prior information can be exploited in attention models and discuss ongoing research efforts and open challenges in the area, providing the first extensive categorization of the vast body of literature in this exciting domain."
Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing,"Pretraining large neural language models, such as BERT, has led to impressive gains on many natural language processing (NLP) tasks. However, most pretraining efforts focus on general domain corpora, such as newswire and Web. A prevailing assumption is that even domain-specific pretraining can benefit by starting from general-domain language models. In this article, we challenge this assumption by showing that for domains with abundant unlabeled text, such as biomedicine, pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models. To facilitate this investigation, we compile a comprehensive biomedical NLP benchmark from publicly available datasets. Our experiments show that domain-specific pretraining serves as a solid foundation for a wide range of biomedical NLP tasks, leading to new state-of-the-art results across the board. Further, in conducting a thorough evaluation of modeling choices, both for pretraining and task-specific fine-tuning, we discover that some common practices are unnecessary with BERT models, such as using complex tagging schemes in named entity recognition. "
Stanza: A Python Natural Language Processing Toolkit for Many Human Languages,"We introduce Stanza, an open-source Python natural language processing toolkit supporting 66 human languages. Compared to existing widely used toolkits, Stanza features a language-agnostic fully neural pipeline for text analysis, including tokenization, multi-word token expansion, lemmatization, part-of-speech and morphological feature tagging, dependency parsing, and named entity recognition. We have trained Stanza on a total of 112 datasets, including the Universal Dependencies treebanks and other multilingual corpora, and show that the same neural architecture generalizes well and achieves competitive performance on all languages tested. Additionally, Stanza includes a native Python interface to the widely used Java Stanford CoreNLP software, which further extends its functionality to cover other tasks such as coreference resolution and relation extraction. "
Transfer Learning in Natural Language Processing,"The classic supervised machine learning paradigm is based on learning in isolation, a single predictive model for a task using a single dataset. This approach requires a large number of training examples and performs best for well-defined and narrow tasks. Transfer learning refers to a set of methods that extend this approach by leveraging data from additional domains or tasks to train a model with better generalization properties. Over the last two years, the field of Natural Language Processing (NLP) has witnessed the emergence of several transfer learning methods and architectures which significantly improved upon the state-of-the-art on a wide range of NLP tasks. These improvements together with the wide availability and ease of integration of these methods are reminiscent of the factors that led to the success of pretrained word embeddings and ImageNet pretraining in computer vision, and indicate that these methods will likely become a common tool in the NLP landscape as well as an important research direction. We will present an overview of modern transfer learning methods in NLP, how models are pre-trained, what information the representations they learn capture, and review examples and case studies on how these models can be integrated and adapted in downstream NLP tasks."
Revisiting Pre-Trained Models for Chinese Natural Language Processing,"Bidirectional Encoder Representations from Transformers (BERT) has shown marvelous improvements across various NLP tasks, and consecutive variants have been proposed to further improve the performance of the pre-trained language models. In this paper, we target on revisiting Chinese pre-trained language models to examine their effectiveness in a non-English language and release the Chinese pre-trained language model series to the community. We also propose a simple but effective model called MacBERT, which improves upon RoBERTa in several ways, especially the masking strategy that adopts MLM as correction (Mac). We carried out extensive experiments on eight Chinese NLP tasks to revisit the existing pre-trained language models as well as the proposed MacBERT. Experimental results show that MacBERT could achieve state-of-the-art performances on many NLP tasks, and we also ablate details with several findings that may help future research."
A Survey of the State of Explainable AI for Natural Language Processing,"Recent years have seen important advances in the quality of state-of-the-art models, but this has come at the expense of models becoming less interpretable. This survey presents an overview of the current state of Explainable AI (XAI), considered within the domain of Natural Language Processing (NLP). We discuss the main categorization of explanations, as well as the various ways explanations can be arrived at and visualized. We detail the operations and explainability techniques currently available for generating explanations for NLP model predictions, to serve as a resource for model developers in the community. Finally, we point out the current gaps and encourage directions for future work in this important research area."
Predictive Biases in Natural Language Processing Models: A Conceptual Framework and Overview,"An increasing number of works in natural language processing have addressed the effect of bias on the predicted outcomes, introducing mitigation techniques that act on different parts of the standard NLP pipeline (data and models). However, these works have been conducted in isolation, without a unifying framework to organize efforts within the field. This leads to repetitive approaches, and puts an undue focus on the effects of bias, rather than on their origins. Research focused on bias symptoms rather than the underlying origins could limit the development of effective countermeasures. In this paper, we propose a unifying conceptualization: the predictive bias framework for NLP. We summarize the NLP literature and propose a general mathematical definition of predictive bias in NLP along with a conceptual framework, differentiating four main origins of biases: label bias, selection bias, model overamplification, and semantic bias. We discuss how past work has countered each bias origin. Our framework serves to guide an introductory overview of predictive bias in NLP, integrating existing work into a single structure and opening avenues for future research."
Continual Lifelong Learning in Natural Language Processing: A Survey,"Continual learning (CL) aims to enable information systems to learn from a continuous data stream across time. However, it is difficult for existing deep learning architectures to learn a new task without largely forgetting previously acquired knowledge. Furthermore, CL is particularly challenging for language learning, as natural language is ambiguous: it is discrete, compositional, and its meaning is context-dependent. In this work, we look at the problem of CL through the lens of various NLP tasks. Our survey discusses major challenges in CL and current methods applied in neural network models. We also provide a critical review of the existing CL evaluation methods and datasets in NLP. Finally, we present our outlook on future research directions."
COVID-Twitter-BERT: A Natural Language Processing Model to Analyse COVID-19 Content on Twitter,"In this work, we release COVID-Twitter-BERT (CT-BERT), a transformer-based model, pretrained on a large corpus of Twitter messages on the topic of COVID-19. Our model shows a 10-30% marginal improvement compared to its base model, BERT-Large, on five different classification datasets. The largest improvements are on the target domain. Pretrained transformer models, such as CT-BERT, are trained on a specific target domain and can be used for a wide variety of natural language processing tasks, including classification, question-answering and chatbots. CT-BERT is optimised to be used on COVID-19 content, in particular social media posts from Twitter."
"Internet of Things (IoT) for Next-Generation Smart Systems: A Review of Current Challenges, Future Trends and Prospects for Emerging 5G-IoT Scenarios","The Internet of Things (IoT)-centric concepts like augmented reality, high-resolution video streaming, self-driven cars, smart environment, e-health care, etc. have a ubiquitous presence now. These applications require higher data-rates, large bandwidth, increased capacity, low latency and high throughput. In light of these emerging concepts, IoT has revolutionized the world by providing seamless connectivity between heterogeneous networks (HetNets). The eventual aim of IoT is to introduce the plug and play technology providing the end-user, ease of operation, remotely access control and configurability. This paper presents the IoT technology from a bird's eye view covering its statistical/architectural trends, use cases, challenges and future prospects. The paper also presents a detailed and extensive overview of the emerging 5G-IoT scenario. Fifth Generation (5G) cellular networks provide key enabling technologies for ubiquitous deployment of the IoT technology. These include carrier aggregation, multiple-input multiple-output (MIMO), massive-MIMO (M-MIMO), coordinated multipoint processing (CoMP), device-to-device (D2D) communications, centralized radio access network (CRAN), software-defined wireless sensor networking (SD-WSN), network function virtualization (NFV) and cognitive radios (CRs). This paper presents an exhaustive review for these key enabling technologies and also discusses the new emerging use cases of 5G-IoT driven by the advances in artificial intelligence, machine and deep learning, ongoing 5G initiatives, quality of service (QoS) requirements in 5G and its standardization issues. Finally, the paper discusses challenges in the implementation of 5G-IoT due to high data-rates requiring both cloud-based platforms and IoT devices based edge computing."
A Review and State of Art of Internet of Things (IoT),"The Internet of Things (IoT) is basically like a system for connecting computer devices, mechanical and digital machines, objects, or individuals provided with the unique system (UIDs) and without transfer to transmit data over an ability human-to-human or computer-to-human relation. Another thing on the internet is that the items in the IoT are like a connected manner with humans and computers to which internet protocol addresses can be assigned and which can transfer data over the network or another man-made object. In this paper, we describe the utilization of IoT in the cloud, fog, IoT technologies with applications and security. Specifically, we provide IoT architecture for design and development with sensors in 6G. Finally, we discuss the current research, solutions, and present open issues of future research in IoT."
"Comprehensive review on ML-based RIS-enhanced IoT systems: basics, research progress and future challenges","Sixth generation (6G) internet of things (IoT) networks will modernize the applications and satisfy user demands through implementing smart and automated systems. Intelligence-based infrastructure, also called reconfigurable intelligent surfaces (RISs), have been introduced as a potential technology striving to improve system performance in terms of data rate, latency, reliability, availability, and connectivity. A huge amount of cost-effective passive components are included in RISs to interact with the impinging electromagnetic waves in a smart way. However, there are still some challenges in RIS system, such as finding the optimal configurations for a large number of RIS components. In this paper, we first provide a complete outline of the advancement of RISs along with machine learning (ML) algorithms and overview the working regulations as well as spectrum allocation in intelligent IoT systems. Also, we discuss the integration of different ML techniques in the context of RIS, including deep reinforcement learning (DRL), federated learning (FL), and FL-deep deterministic policy gradient (FL-DDPG) techniques which are utilized to design the radio propagation atmosphere without using pilot signals or channel state information (CSI). Additionally, in dynamic intelligent IoT networks, the application of existing integrated ML solutions to technical issues like user movement and random variations of wireless channels are surveyed. Finally, we present the main challenges and future directions in integrating RISs and other prominent methods to be applied in upcoming IoT networks."
"Key communication technologies, applications, protocols and future guides for IoT-assisted smart grid systems: A review","Towards addressing the concerns of conventional power systems including reliability and security, establishing modern Smart Grids (SGs) has been given much attention by the global electric utility applications during the last few years. One of the key advantageous of SGs is its ability for two-way communication and bi-directional power flow that facilitates the inclusion of distributed energy resources, real time monitoring and self-healing systems. As such, the SG employs a large number of digital devices that are installed at various locations to enrich the observability and controllability of the system. This calls for the necessity of employing Internet of Things (IoT) to achieve reliable integration of all digital devices and proper tracing of various apparatuses in the grid. In this paper, the communication technology, architectural design, cutting-edge applications, and protocols of IoT-assisted SG systems are comprehensively reviewed. The main concerns, future challenges, and research gaps of IoT-assisted smart grid systems are highlighted in detail. Based on this review, key findings are concluded to pave the path for further research directions on IoT-assisted SG systems."
Integration of IoT in building energy infrastructure: A critical review on challenges and solutions,"The Internet of Things (IoT) has unprecedentedly entangled the physical world with cyber technologies and its integration with building infrastructure (BI) is no different. Integration of IoT can impart BI with upscale features like remote operations, automated management and user-centric facilities by developing an interconnected cognitive building (CB) ecosystem. However, this integration has entered an ambiguous phase of realizing mature adoption and practical utilization of IoT in BI for both academic and industrial domains. This ambiguity restricts the IoT and BI stakeholders to comprehend and acknowledge the full operational competency of IoT in BI. Thus, a significant research gap exists that deeply investigates the practical implementation and mature adoption of IoT in BI. The prime objective of this study is to establish a comprehensive review by exploring the state-of-art academic, technological and industrial research to identify major technological and behavioural interventions that successfully enhance the integration of IoT in BI. Besides, this study also highlights significant technical and non-technical challenges that require substantial research efforts for maturing the adoption of IoT in BI. The findings of the study argue that the full operational competency of IoT in BI is not yet realized and a dedicated effort from both IoT and BI stakeholders is required to provide modern BI with a generic IoT framework having cognitive intelligence and context-aware computing capabilities. The proposed study will assist the researchers in realizing the full operational competency of IoT in BI for more exciting innovations."
Secure Intelligent Fuzzy Blockchain Framework: Effective Threat Detection in IoT Networks,"Integrating blockchain into the Internet of Things (IoT) for security is a new development in computational communication systems. While security threats are changing their strategies and constructing new threats on blockchain-based IoT systems. Also, in combining blockchain with IoT networks, malicious transactions and active attacks deliver more vulnerabilities, privacy issues, and security threats. The concept of blockchain-based IoT attacks is a hot topic in both IoT and blockchain disciplines. Network attacks are a type of security and privacy threat and cover the exact scope of threats related to the combination of IoT and blockchain. Even though blockchain has potential security benefits, new cyberattacks have emerged that make blockchain alone insufficient to deal with threats and attacks in IoT networks since vagueness and ambiguity issues are unavoidable in IoT data. The heterogeneous nature of IoT sources has made uncertainty a critical issue in IoT networks. Deep Learning (DL) models have difficulty dealing with uncertainty issues and cannot manage them efficiently as an essential tool in security techniques. Thus, we need better security, privacy, and practical approaches, such as efficient threat detection against network attacks in blockchain-based IoT environments. Also helpful to consider fuzzy logic to tackle deterministic issues when DL models face uncertainty. This paper designs and implements a secure, intelligent fuzzy blockchain framework. This framework utilizes a novel fuzzy DL model, optimized adaptive neuro-fuzzy inference system (ANFIS)-based attack detection, fuzzy matching (FM), and fuzzy control system (FCS) for detection of network attacks. The proposed fuzzy DL applies the fuzzy Choquet integral to have a powerful nonlinear aggregation function in the detection. We use metaheuristic algorithms to optimize the attack detection error function in ANFIS. We also validate transactions via FM to tackle fraud detection and efficiency in the blockchain layer. This framework is the first secure, intelligent fuzzy blockchain framework that identifies and detects security threats while considering uncertainty issues in IoT networks and having more flexibility in decision-making and accepting transactions in the blockchain layer. Evaluation results verify the efficiency of the blockchain layer in throughput and latency metrics and the intelligent fuzzy layer in performance metrics (Accuracy, Precision, Recall, and F1-Score) in threat detection on both blockchain and IoT network sides. Additionally, FCS demonstrates that we obtain an effective system (stable model) for threat detection in blockchain-based IoT networks."
AoI-aware energy control and computation offloading for industrial IoT,"In Industrial Internet of Things (IIoT), a large volume of data is collected periodically by IoT devices, and timely data routing and processing are important requirements. Age of Information (AoI), which is a metric to evaluate the freshness of status information in data processing, has become one of the most important objectives in IIoT. In this paper, considering limited communication, computation and energy resources on IoT devices, we jointly study the optimal AoI-aware energy control and computation offloading problem within a dynamic IIoT scenario with multiple IoT devices and multiple edge servers. Based on extensive analysis of real-life IoT dataset, Markovian queueing models are constructed to capture the dynamics of IoT devices and edge servers, and their corresponding analyses are provided. With the quantitative analytical results, we formulate a dynamic Markov decision problem with the objective of minimizing the long-term energy consumption while satisfying AoI constraints for real-time data processing. To solve the problem, we apply Deep Reinforcement Learning (DRL) techniques for adapting to large-scale dynamic IIoT environments, and design an intelligent Energy Control and Computation Offloading (ECCO) algorithm. Extensive simulation experiments are conducted based on real-world dataset, and the comparison results illustrate the superiority of our ECCO algorithm over both existing DRL and non-DRL algorithms."
Enhancement of Online Education in Engineering College Based on Mobile Wireless Communication Networks and IOT.," The field of Engineering is that which needs a high level of analytical thinking, intuitive knowledge, and technical know-how. The area of communication engineering deals with different components including, wireless mobile services, radio, broadband, web and satellites. There is a rapid decline in the quality of students produced by engineering faculties as a result of sufficient and quality methods and frameworks of student assessment. The production of high-potential engineers is limited by the utilization of old and traditional education methodology and frameworks. The student presentation estimation system in engineering institution is a motionless manual. Usually, the assessment of student's performance using the traditional system is limited to the use of students' performance scores, while failing to evaluate their performance based on activities or practical applications. In addition, such systems do not take cognizance of individual knowledge of students that connects to different activities within the learning environment. Recently, engineering institutions have started paying attention to evaluation solutions that are based on wireless networks and Internet of Things (IoT). Therefore, in this study, an automated system has been proposed for the assessment of engineering students. The proposed system is designed based on IoT and wireless communication networks with the aim of improving the process of virtual education. The data used in this study has been collected through the use of different IoT sensors within the premises of the college, and pre-processed using normalization. After the data was pre-processed, it was stored in cloud. In order to enable the classification of student's activity, an Adaptive Layered Bayesian Belief Network (AL-BBN) classifier is proposed in this work. The student's scores have been calculated using fuzzy logic, while Multi-Gradient Boosting Decision Tree (MGBDT) was proposed for decision making. The use of python simulation tool is employed in the implementation of the proposed system, and the evaluation of the performance benchmarks was done as well. Based on the findings of the study, the proposed conceptual model outperformed the existing ones in terms of improving the process of online learning."
Data secure transmission intelligent prediction algorithm for mobile industrial IoT networks,"Mobile Industrial Internet of Things (IIoT) applications have achieved the explosive growth in recent years. The mobile IIoT has flourished and become the backbone of the industry, laying a solid foundation for the interconnection of all things. The variety of application scenarios has brought serious challenges to mobile IIoT networks, which face complex and changeable communication environments. Ensuring data secure transmission is critical for mobile IIoT networks. This paper investigates the data secure transmission performance prediction of mobile IIoT networks. To cut down computational complexity, we propose a data secure transmission scheme employing Transmit Antenna Selection (TAS). The novel secrecy performance expressions are first derived. Then, to realize real-time secrecy analysis, we design an improved Convolutional Neural Network (CNN) model, and propose an intelligent data secure transmission performance prediction algorithm. For mobile signals, the important features may be removed by the pooling layers. This will lead to negative effects on the secrecy performance prediction. A novel nine-layer improved CNN model is designed. Out of the input and output layers, it removes the pooling layer and contains six convolution layers. Elman, Back-Propagation (BP) and LeNet methods are employed to compare with the proposed algorithm. Through simulation analysis, good prediction accuracy is achieved by the CNN algorithm. The prediction accuracy obtains a 59% increase."
PatrIoT: practical and agile threat research for IoT,"The Internet of things (IoT) products, which have been widely adopted, still pose challenges in the modern cybersecurity landscape. Many IoT devices are resource-constrained and almost constantly online. Furthermore, the security features of these devices are less often of concern, and fewer methods, standards, and guidelines are available for testing them. Although a few approaches are available to assess the security posture of IoT products, the ones in use are mostly based on traditional non-IoT-focused techniques and generally lack the attackers’ perspective. This study provides a four-stage IoT vulnerability research methodology built on top of four key elements: logical attack surface decomposition, compilation of top 100 weaknesses, lightweight risk scoring, and step-by-step penetration testing guidelines. Our proposed methodology is evaluated with multiple IoT products. The results indicate that PatrIoT allows cyber security practitioners without much experience to advance vulnerability research activities quickly and reduces the risk of critical IoT penetration testing steps being overlooked."
IoT-based application for construction site safety monitoring,"Hong Kong construction safety has witnessed substantial improvement in the last three decades, however, accidents still occur frequently as more than 4,000 accidents are reported in the year 2017. Against this background, this research, firstly, aims to investigate the effectiveness of safety training for construction personnel in Hong Kong. A questionnaire is designed to explore the efficacy and weaknesses of mandatory basic safety training. The results indicate the inadequate knowledge of the concept of personal protective equipment as the main weakness of the workers. Secondly, to overcome the training weakness, an Internet-of-Things (IoT) based innovative safety model is designed to provide real-time monitoring of construction site personnel and environment. The proposed model not only identifies real-time personnel safety problems, i.e., near misses, to reduce the accident rates but also stores the digital data to improve future training and system itself. The proposed model in this research provides a cost-effective solution for optimal construction safety to the stakeholders. A cost comparison analysis suggests that the IoT system can provide 1) 78% cost-savings with respect to the traditional manual system and 2) 65% cost-savings with respect to the traditional sensor system."
"Role of emerging technologies in future IoT-driven Healthcare 4.0 technologies: a survey, current challenges and future directions","Since its inception, Healthcare 4.0 has empowered the integration of advanced technologies to create and improve the quality of healthcare services. The delivery of healthcare services has come a long way from physical appointments with doctors to remote health monitoring and disease prediction, surgery assistive systems. This advancement has only been possible because of the integration of cutting-edge technologies like Tele-healthcare, software-defined networking and many more, with healthcare systems. In this survey, we have targeted some of the pioneering research works that could contribute significantly to the future development of Healthcare 4.0 systems. We have identified the significant research gaps and presented the modern state-of-the-art of healthcare systems, introducing the Healthcare IoT Application and Service Stacks. We have also discussed the latest paradigm of Wireless Body Area Networks, emphasizing its significance and how it can contribute to the development of next-generation healthcare applications using emerging technologies like Machine Learning, Blockchain, Cloud Computing, Internet of things, Edge/ Fog Computing, Tele-healthcare, Big Data Analytics, Software-Defined Networking and many more. We have performed a comparative study of different architectural implementations considering their advantages, shortcomings, and quality-of-service requirements. We emphasize the importance of the different emerging technologies in detail, discussing the opportunities available and their potential to create better healthcare solutions that can provide superior service quality. Finally, we highlight the fundamental need for establishing security and privacy in future healthcare systems. Overall, this survey provides a strong outlook into the development of the future of healthcare 4.0."
ASSIST-IoT: A Modular Implementation of a Reference Architecture for the Next Generation Internet of Things,"Next Generation Internet of Things (NGIoT) addresses the deployment of complex, novel IoT ecosystems. These ecosystems are related to different technologies and initiatives, such as 5G/6G, AI, cybersecurity, and data science. The interaction with these disciplines requires addressing complex challenges related with the implementation of flexible solutions that mix heterogeneous software and hardware, while providing high levels of customisability and manageability, creating the need for a blueprint reference architecture (RA) independent of particular existing vertical markets (e.g., energy, automotive, or smart cities). Different initiatives have partially dealt with the requirements of the architecture. However, the first complete, consolidated NGIoT RA, covering the hardware and software building blocks, and needed for the advent of NGIoT, has been designed in the ASSIST-IoT project. The ASSIST-IoT RA delivers a layered and modular design that divides the edge-cloud continuum into independent functions and cross-cutting capabilities. This contribution discusses practical aspects of implementation of the proposed architecture within the context of real-world applications. In particular, it is shown how use of cloud-native concepts (microservices and applications, containerisation, and orchestration) applied to the edge-cloud continuum IoT systems results in bringing the ASSIST-IoT concepts to reality. The description of how the design elements can be implemented in practice is presented in the context of an ecosystem, where independent software packages are deployed and run at the selected points in the hardware environment. Both implementation aspects and functionality of selected groups of virtual artefacts (micro-applications called enablers) are described, along with the hardware and software contexts in which they run."
"Vaccine supply chain management: An intelligent system utilizing blockchain, IoT and machine learning","Vaccination offers health, economic, and social benefits. However, three major issues—vaccine quality, demand forecasting, and trust among stakeholders—persist in the vaccine supply chain (VSC), leading to inefficiencies. The COVID-19 pandemic has exacerbated weaknesses in the VSC, while presenting opportunities to apply digital technologies to manage it. For the first time, this study establishes an intelligent VSC management system that provides decision support for VSC management during the COVID-19 pandemic. The system combines blockchain, internet of things (IoT), and machine learning that effectively address the three issues in the VSC. The transparency of blockchain ensures trust among stakeholders. The real-time monitoring of vaccine status by the IoT ensures vaccine quality. Machine learning predicts vaccine demand and conducts sentiment analysis on vaccine reviews to help companies improve vaccine quality. The present study also reveals the implications for the management of supply chains, businesses, and government."
Integration of IoT into e-government,"The integration of IoT in e-government is a novel and weakly explored concept, particularly in the light of new advances such as blockchain in the e-government, which requires further exploration and conceptualization, thereby achieving a shared/common vision and body of knowledge for its further successful and sustainable adoption – to the best of the authors’ knowledge, the current study is one of these initial attempts."
A comprehensive study of DDoS attacks over IoT network and their countermeasures,"IoT offers capabilities to gather information from digital devices, infer from their results, and maintain and optimize these devices in different domains. IoT is heterogeneous in nature, which makes it prone to various security threats like confidentiality and integrity breaches, lack of availability of resources, trust issues, etc. The security concerns lead to different attacks over the system, and the Distributed Denial of Services (DDoS) bout is growing generously. DDoS is an assault that targets the availability of resources and servers of a network by flooding the communication medium from distinct locations by utilizing various IoT devices, which makes it harder to detect. Thus, analyzing and defending DDoS is a protruding field of research these days. The paper gives a thorough knowledge of DDoS over IoT. In this, we have critically analysed the existing DDoS variants, IoT Security issues, the execution of DDoS attempts, along with the exploitation of IoT devices and creation of them in Botnets or zombies. Moreover, the paper will also cover prevailing DDoS defense methodologies as well as their comparative analysis for ease of understanding."
Robust fault recognition and correction scheme for induction motors using an effective IoT with deep learning approach,"Maintaining electrical machines in good working order and increasing their life expectancy is one of the main challenges. Precocious and accurate detection of faults is crucial to this process. Induction motors (IMs) are among these machines widely utilized in various fields including industrial and domestic applications that require effective detection of their status. This paper proposes a novel fault recognition and correction (FRC) scheme based on the internet of things (IoT) and deep learning for IMs. In the developed system, vibration signals during motor operation are used to generate bearing fault features, which are inputted to the designed deep learning model to successfully identify bearing faults. The robustness of the proposed approach is tested against a false data injection (FDI) attack. Further, the proposed deep learning approach has been assessed and compared with other state-of-the-art algorithms available in the literature. Experimental testing has been carried out on a real IM to perform the suitability of the developed fault detection and correction scheme. Compared to other fault recognition techniques, the proposed method proves to be more effective. In essence, the results verified the robustness of the novel proposed strategy against the FDI attack making it possible to recognize faults with confidence and improve decision-making to determine the motor's status."
Towards an Interdisciplinary Development of IoT-Enhanced Business Processes,"IoT-enhanced Business Processes make use of sensors and actuators to carry out the process tasks and achieve a specific goal. One of the most important difficulties in the development of IoT-enhanced BPs is the interdisciplinarity that is demanded by this type of project. Defining an interdisciplinary tool-supported development approach that facilitates the collaboration of different professionals, with a special focus on three main facets: business process requirements, interoperability between IoT devices and BPs, and low-level data processing. The study followed a Design Science Research methodology for information systems that consists of a 6-step process: (1) problem identification and motivation; (2) define the objectives for a solution; (3) design and development; (4) demonstration; (5) evaluation; and (6) communication. The paper presents an interdisciplinary development process to support the creation of IoT-enhanced BPs by applying the Separation of Concerns principle. A collaborative development environment is built to provide each professional with the tools required to accomplish her/his development responsibilities. The approach is successfully validated through a case-study evaluation. The evaluation allows to conclude that the proposed development process and the supporting development environment are effective to face the interdisciplinary nature of IoT-enhanced BPs."
Semantic models for IoT sensing to infer environment–wellness relationships,"Every time an Internet of Things (IoT) solution is deployed, every time a smartphone owner connects her/his wireless device to a wearable activity-tracker, every time groups of citizens use geo-mapping applications to move around the city, choosing the least crowded path, data are produced and information have to be exchanged appropriately via APIs. Even if novel added-value IoT-based applications appear on the market with increasing speed, true semantic interoperability is far from being achieved, thus limiting the large-scale exploitation, the scalability and the time-to-market of novel apps. Currently, connecting different data prosumers with multiple data sources is still hampered by the lack of standardized and sustainable solutions, especially due to the significant heterogeneity of IoT platforms. In such a landscape, ontologies come to the rescue, thanks to their formal semantics, knowledge representation formats, and shared vocabularies. In this paper we examine, from an ontological perspective, how to describe environmental sensing and wellness monitoring, two of the most popular application cases of Mobile Crowd Sensing (MCS) and IoT, respectively. To this purpose, an ontology of sensor-agnostic APIs is proposed, along with a set of MCS-dedicated ontology modules (and the supporting platform), leveraging on standard and reusable domain ontologies. Moreover, it will be shown how to properly combine the proposed ontologies in order to support complex functionalities based on inference rules addressing the environment–wellness relationships. Finally, specific semantic modeling patterns suitable for typical IoT and MCS scenarios will be discussed."
A blockchain-orchestrated deep learning approach for secure data transmission in IoT-enabled healthcare system,"The integration of the Internet of Things (IoT) with traditional healthcare systems has improved quality of healthcare services. However, the wearable devices and sensors used in Healthcare System (HS) continuously monitor and transmit data to the nearby devices or servers using an unsecured open channel. This connectivity between IoT devices and servers improves operational efficiency, but it also gives a lot of room for attackers to launch various cyber-attacks that can put patients under critical surveillance in jeopardy. In this article, a Blockchain-orchestrated Deep learning approach for Secure Data Transmission in IoT-enabled healthcare system hereafter referred to as “BDSDT” is designed. Specifically, first a novel scalable blockchain architecture is proposed to ensure data integrity and secure data transmission by leveraging Zero Knowledge Proof (ZKP) mechanism. Then, BDSDT integrates with the off-chain storage InterPlanetary File System (IPFS) to address difficulties with data storage costs and with an Ethereum smart contract to address data security issues. The authenticated data is further used to design a deep learning architecture to detect intrusion in HS network. The latter combines Deep Sparse AutoEncoder (DSAE) with Bidirectional Long Short-Term Memory (BiLSTM) to design an effective intrusion detection system. Experiments on two public data sources (CICIDS-2017 and ToN-IoT) reveal that the proposed BDSDT outperformed state-of-the-arts in both non-blockchain and blockchain settings and have obtained accuracy close to 99% using both datasets."
"Fusion of blockchain and IoT in scientific publishing: Taxonomy, tools, and future directions","Scientific publishing systems (SPS) provides platforms to authors, reviewers, and editors to express research for the betterment of the community. Traditionally, the research databases are assigned electronic identifiers, and manuscripts are preserved in electronic form. Owing to the large scale of submissions in the databases, it becomes difficult for the repositories to manage their electronic volumes. The search queries and retrievals are complex, and the publishing process takes a lot of time, which defeats the purpose of the contribution in many cases by the author. Moreover, the process is non-transparent, and is limited due to ineffective article tracking policies. With the advent of the Internet of Things (IoT), the libraries have transitioned towards smart objects that process academic repositories with low-powered computations. In the same way, meta-information passes through lightweight IoT protocols to distributed servers. Coupled with blockchain (BC), a secured and trusted publishing platform is assured in SPS, with transparency among all academic stakeholders. Traditional SPS platforms do not provide any rewarding method for peer review and do not support and store unsuccessful articles. Besides, published works are not verified thoroughly, and this can lead to misconduct in scientific publishing. Motivated by these facts, in this paper, we present a survey on the fusion of BC and IoT for SPS, which serves the dual purpose of low-powered computational tagging of manuscripts as smart objects, and that also supports rewarding and completing the verification of transactions by peers without involving a third party. A case study of a hyperledger driven IoT-enabled scientific publishing system (SPS) is proposed to address the limitations of the traditional SPS. Lastly, we present open issues and challenges concerning the current SPSs and the proposed BC-driven SPS."
Landscape of IoT security,"The last two decades have experienced a steady rise in the production and deployment of sensing-and-connectivity-enabled electronic devices, replacing “regular” physical objects. The resulting Internet-of-Things (IoT) will soon become indispensable for many application domains. Smart objects are continuously being integrated within factories, cities, buildings, health institutions, and private homes. Approximately 30 years after the birth of IoT, society is confronted with significant challenges regarding IoT security. Due to the interconnectivity and ubiquitous use of IoT devices, cyberattacks have widespread impacts on multiple stakeholders. Past events show that the IoT domain holds various vulnerabilities, exploited to generate physical, economic, and health damage. Despite many of these threats, manufacturers struggle to secure IoT devices properly. Thus, this work overviews the IoT security landscape with the intention to emphasize the demand for secured IoT-related products and applications. Therefore, (a) a list of key challenges of securing IoT devices is determined by examining their particular characteristics, (b) major security objectives for secured IoT systems are defined, (c) a threat taxonomy is introduced, which outlines potential security gaps prevalent in current IoT systems, and (d) key countermeasures against the aforementioned threats are summarized for selected IoT security-related technologies available on the market."
Review of agricultural IoT technology,"Agricultural Internet of Things (IoT) has brought new changes to agricultural production. It not only increases agricultural output but can also effectively improve the quality of agricultural products, reduce labor costs, increase farmers' income, and truly realize agricultural modernization and intelligence. This paper systematically summarizes the research status of agricultural IoT. Firstly, the current situation of agricultural IoT is illustrated and its system architecture is summarized. Then, the five key technologies of agricultural IoT are discussed in detail. Next, applications of agricultural IoT in five representative fields are introduced. Finally, the problems existing in agricultural IoT are analyzed and a forecast is given of the future development of agricultural IoT."
A Review on Deep Learning Techniques for IoT Data,"Continuous growth in software, hardware and internet technology has enabled the growth of internet-based sensor tools that provide physical world observations and data measurement. The Internet of Things(IoT) is made up of billions of smart things that communicate, extending the boundaries of physical and virtual entities of the world further. These intelligent things produce or collect massive data daily with a broad range of applications and fields. Analytics on these huge data is a critical tool for discovering new knowledge, foreseeing future knowledge and making control decisions that make IoT a worthy business paradigm and enhancing technology. Deep learning has been used in a variety of projects involving IoT and mobile apps, with encouraging early results. With its data-driven, anomaly-based methodology and capacity to detect developing, unexpected attacks, deep learning may deliver cutting-edge solutions for IoT intrusion detection. In this paper, the increased amount of information gathered or produced is being used to further develop intelligence and application capabilities through Deep Learning (DL) techniques. Many researchers have been attracted to the various fields of IoT, and both DL and IoT techniques have been approached. Different studies suggested DL as a feasible solution to manage data produced by IoT because it was intended to handle a variety of data in large amounts, requiring almost real-time processing. We start by discussing the introduction to IoT, data generation and data processing. We also discuss the various DL approaches with their procedures. We surveyed and summarized major reporting efforts for DL in the IoT region on various datasets. The features, application and challenges that DL uses to empower IoT applications, which are also discussed in this promising field, can motivate and inspire further developments."
Internet of Things (IoT): From awareness to continued use,"This paper proposes a research model with five constructs, i.e., IoT awareness, users’ IoT privacy knowledge, users’ IoT security knowledge, users’ IoT Trust, and continued intention to use IoT to bring clarity to the growing yet fragmented literature on the path from IoT awareness to the continued use of IoT. Hypotheses stemming from the proposed model were stated. A total of 297 subjects from various organizations in 9 regions of the USA participated in the study. Collected data were analyzed through path modeling, using SmartPLS 3.0. The results indicated that IoT awareness can positively influence users’ knowledge of IoT privacy and security. The users’ knowledge of IoT privacy and security can positively influence users’ IoT trust and subsequently, the users’ IoT trust can positively influence continued intention to use IoT. Additionally, IoT privacy knowledge, IoT security knowledge, and IoT trust were found to be the mediating variables in the proposed model. Theoretical and practical implications of findings, as well as recommendations for further research, are discussed."
Applications of IoT for optimized greenhouse environment and resources management,"The role of Internet-of-Things (IoT) in precision agriculture and smart greenhouses has been reinforced by recent R&D projects, growing commercialization of IoT infrastructure, and related technologies such as satellites, artificial intellige nce, sensors, actuators, uncrewed aerial vehicles, big data analytics, intelligent machines, and radio-frequency identification devices. Even though the integration of intelligent technologies offers unlimited potential in precision commercial agriculture, optimal resource management remains a challenge considering that IoT infrastructure is unevenly distributed across the world and concentrated in high-income countries. The utilization of IoT technologies in smart greenhouses often involves a tradeoff between the cost of agricultural production, environmental conservation, ecological degradation, and sustainability. The installation of IoT infrastructure is capital-intensive and often translates to higher energy demand, that elevates the risk for climate change. The widespread use of IoT sensors and networks also increases new challenges in the management of electronic waste, depletion of finite resources, and destruction of fragile ecosystems, resulting in climate change. the integration of IoT systems in greenhouses would be augmented by the global deployment of advanced 5G technology and Low-Earth Orbit (LEO) constellation broadband internet with low latency and high speeds. Intelligent application of agrochemicals could yield significant savings ($500/acre or more), while need-based irrigation and fertilizer application would help improve crop yields. Globally, the deployment of IoT infrastructure would yield about $500 billion of added value to the GDP by 2030. The forecasted economic benefits affirm that the applications of IoT for optimized greenhouse environment and resources management were sustainable, and any potential risks are incomparable to the long-term benefits in commercial agriculture. The review article contributes new insights on the role of IoT in agriculture 4.0, the challenges, and future prospects for developing nations, which lacked the resources to invest in precision agriculture technologies."
"IoT-Enabled Smart Agriculture: Architecture, Applications, and Challenges","The growth of the global population coupled with a decline in natural resources, farmland, and the increase in unpredictable environmental conditions leads to food security is becoming a major concern for all nations worldwide. These problems are motivators that are driving the agricultural industry to transition to smart agriculture with the application of the Internet of Things (IoT) and big data solutions to improve operational efficiency and productivity. The IoT integrates a series of existing state-of-the-art solutions and technologies, such as wireless sensor networks, cognitive radio ad hoc networks, cloud computing, big data, and end-user applications. This study presents a survey of IoT solutions and demonstrates how IoT can be integrated into the smart agriculture sector. To achieve this objective, we discuss the vision of IoT-enabled smart agriculture ecosystems by evaluating their architecture (IoT devices, communication technologies, big data storage, and processing), their applications, and research timeline. In addition, we discuss trends and opportunities of IoT applications for smart agriculture and also indicate the open issues and challenges of IoT application in smart agriculture. We hope that the findings of this study will constitute important guidelines in research and promotion of IoT solutions aiming to improve the productivity and quality of the agriculture sector as well as facilitating the transition towards a future sustainable environment with an agroecological approach."
Intelligent ecofriendly transport management system based on IoT in urban areas,"Due to the large amount of private vehicle traffic, which has also ultimately resulted in massively complicated traffic in urban areas, transportation has become one of the most common characteristics of everyday life because the large amount of congestion has a number of negative consequences, including increased consumption of fossil fuels, pollution, unanticipated collisions, and loss of time. IoT technology has quickly added an effective and proper traffic management system, notably in road transport, due to the tremendous features IoT can manage, such as organizing, monitoring, attempting to identify, informatics, and so on. This article provides an idea of a few smart management systems that have developed to help with traffic congestion reduction via the IoT."
A review paper on wireless sensor network techniques in Internet of Things (IoT),"In recent times, it has been witnessed that wireless systems based on IoT-based have developed rapidly in various sectors. The IoT (Internet of Things) is the network in which physical devices, equipment, sensors and other objects can communicate among themselves without human involvement. The WSN (Wireless Sensor Network) is a central component of the IoT, which has proliferated into several different applications in real-time. The IoT and WSNs now have various critical and non-critical applications impacting nearly every area of our everyday life. WSN nodes are usually small and battery-driven machines. Thus, the energy effective data aggregation techniques that increase the lifespan of the network are highly significant. Various approaches and algorithms for energy-efficient data aggregation in IoT-WSN systems were presented. This paper reviews the literature with specific attention to aspects of wireless networking for the preservation of energy and aggregation of data."
Review of RFID and IoT integration in supply chain management,"Supply Chain Management (SCM) is increasingly complex and dynamic. Radio frequency identification (RFID) and the Internet of Things (IoT) are expected to play a significant role in fulfilling customer requirements in the supply chain. In this work, the integration of RFID with IoT is called RFID-IoT. RFID-IoT strives to develop automated sensing, seamless, interoperable and highly secure systems by connecting IoT devices through the internet. In this paper, the authors have systematically reviewed the selected literature on the application of RFID-IoT in supply chain management. The contribution of this paper is to review the current state-of-the-art literature and potential trend on the application of RFID-IoT in SCM. There is a need for an in-depth, comprehensive analysis of up-to-date literature to help enhance the management system efficiency, maximize productivity, and minimize cost. This work also explores the current challenges of the reviewed papers in RFID-IoT implementation in the supply chain. The conceptual framework model has been conducted from four major critical SCM perspectives: product manufacturing, shipping and distribution, inventory and retail shop. In the future, this review's highlighted insights and recommendations will hopefully lead to increased efforts toward developing RFID-IoT technologies."
Design and Development of RNN Anomaly Detection Model for IoT Networks,"Cybersecurity is important today because of the increasing growth of the Internet of Things (IoT), which has resulted in a variety of attacks on computer systems and networks. Cyber security has become an increasingly difficult issue to manage as various IoT devices and services grow. Malicious traffic identification using deep learning techniques has emerged as a key component of network intrusion detection systems (IDS). Deep learning methods have been a research focus in network intrusion detection. A Recurrent Neural Network (RNN) is useful in a wide range of applications. First, this paper proposes a novel deep learning model for anomaly detection in IoT networks using a recurrent neural network. Long Short Term Memory (LSTM), BiLSTM, and Gated Recurrent Unit (GRU) techniques are used to implement the proposed model for anomaly detection in IoT networks. A Convolutional Neural Network (CNN) can analyze input features without losing important information, making them particularly well suited for feature learning. Next, a hybrid deep learning model was proposed using convolutional and recurrent neural networks. Finally, a lightweight deep learning model for binary classification was proposed using LSTM, BiLSTM, and GRU based approaches. The proposed deep learning models are validated using NSLKDD, BoT-IoT, IoT-NI, IoT-23, MQTT, MQTTset, and IoT-DS2 datasets. Compared to current deep learning implementations, the proposed multiclass and binary classification model achieved high accuracy, precision, recall, and F1 score."
Attention mechanisms in computer vision: A survey,"Humans can naturally and effectively find salient regions in complex scenes. Motivated by this observation, attention mechanisms were introduced into computer vision with the aim of imitating this aspect of the human visual system. Such an attention mechanism can be regarded as a dynamic weight adjustment process based on features of the input image. Attention mechanisms have achieved great success in many visual tasks, including image classification, object detection, semantic segmentation, video understanding, image generation, 3D vision, multimodal tasks, and self-supervised learning. In this survey, we provide a comprehensive review of various attention mechanisms in computer vision and categorize them according to approach, such as channel attention, spatial attention, temporal attention, and branch attention; a related repository https://github.com/MenghaoGuo/Awesome-Vision-Attentions is dedicated to collecting related work. We also suggest future directions for attention mechanism research."
Deep reinforcement learning in computer vision: a comprehensive survey,"Deep reinforcement learning augments the reinforcement learning framework and utilizes the powerful representation of deep neural networks. Recent works have demonstrated the remarkable successes of deep reinforcement learning in various domains including finance, medicine, healthcare, video games, robotics, and computer vision. In this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer vision. We start with comprehending the theories of deep learning, reinforcement learning, and deep reinforcement learning. We then propose a categorization of deep reinforcement learning methodologies and discuss their advantages and limitations. In particular, we divide deep reinforcement learning into seven main categories according to their applications in computer vision, i.e. (i) landmark localization (ii) object detection; (iii) object tracking; (iv) registration on both 2D image and 3D image volumetric data (v) image segmentation; (vi) videos analysis; and (vii) other applications. Each of these categories is further analyzed with reinforcement learning techniques, network design, and performance. Moreover, we provide a comprehensive analysis of the existing publicly available datasets and examine source code availability. Finally, we present some open issues and discuss future research directions on deep reinforcement learning in computer vision."
Computer vision and machine learning applied in the mushroom industry: A critical review,"Abstract Background Mushrooms are popular food items containing numerous vitamins, dietary fibers, and a large number of proteins. As a result, mushrooms can increase the body’s immunity and prevent many types of cancer to keep the body healthy. For these reasons, the demand for high yields and safety in the production of high-quality mushrooms is increasing. Scope and approach This review highlights the application of computer vision and machine learning algorithms in the mushroom industry. Through a systematic review of papers published between 1991 and 2021, this article introduces key aspects related to mushrooms (e.g., species identification and quality classification based on artificial intelligence), and discusses the advantages and disadvantages of various approaches. Key findings and conclusions Numerous artificial intelligence and machine vision technologies have been implemented in research efforts focusing on edible fungi. However, their applications are generally limited to the identification of poisonous mushrooms according to their forms, the plucking of cultivated mushrooms covered by soil, and the mechanized grading of mushrooms. Clearly, the currently available methods cannot meet the requirements of the digitization and intelligentization in the field of edible mushrooms. Considering these reasons, it is possible to develop further application opportunities, such as digital mushroom phenotype determination, and high-throughput breeding based on big data, and mechanical picking by a harvesting robot as well. Therefore, the integration of computer vision and machine learning with the development of more efficient algorithms will undoubtedly be a hotspot for future studies in the context of the mushroom industry."
A JAX Library for Computer Vision Research and Beyond,"Scenic is an open-source (https://github.com/google-research/scenic) JAX library with a focus on transformer-based models for computer vision research and beyond. The goal of this toolkit is to facilitate rapid experimentation, prototyping, and research of new architectures and models. Scenic supports a diverse range of tasks (e.g., classification, segmentation, detection) and facilitates working on multi-modal problems, along with GPU/TPU support for large-scale, multi-host and multi-device training. Scenic also offers optimized implementations of state-of-the-art research models spanning a wide range of modalities. Scenic has been successfully used for numerous projects and published papers and continues serving as the library of choice for rapid prototyping and publication of new research ideas."
Computer vision for solid waste sorting: A critical review of academic research,"Waste sorting is highly recommended for municipal solid waste (MSW) management. Increasingly, computer vision (CV), robotics, and other smart technologies are used for MSW sorting. Particularly, the field of CV-enabled waste sorting is experiencing an unprecedented explosion of academic research. However, little attention has been paid to understanding its evolvement path, status quo, and prospects and challenges ahead. To address the knowledge gap, this paper provides a critical review of academic research that focuses on CV-enabled MSW sorting. Prevalent CV algorithms, in particular their technical rationales and prediction performance, are introduced and compared. The distribution of academic research outputs is also examined from the aspects of waste sources, task objectives, application domains, and dataset accessibility. The review discovers a trend of shifting from traditional machine learning to deep learning algorithms. The robustness of CV for waste sorting is increasingly enhanced owing to the improved computation powers and algorithms. Academic studies were unevenly distributed in different sectors such as household, commerce and institution, and construction. Too often, researchers reported some preliminary studies using simplified environments and artificially collected data. Future research efforts are encouraged to consider the complexities of real-world scenarios and implement CV in industrial waste sorting practice. This paper also calls for open sharing of waste image datasets for interested researchers to train and evaluate their CV algorithms."
Classification of insulators using neural network based on computer vision,"Insulators of the electrical power grid are usually installed outdoors, so they suffer from environmental stresses, such as the presence of contamination. Contamination can increase surface conductivity, which can lead to system failures, reducing the reliability of the network. The identification of insulators that have their properties compromised is important so that there are no discharges through its insulating body. To perform the classification of contaminated insulators, this paper presents computer vision techniques for the extraction of contamination characteristics, and a neural network (NN) model for the classification of this condition. Specifically, the Sobel edge detector, Canny edge detection, binarization with threshold, adaptive binarization with threshold, threshold with Otsu and Riddler–Calvard techniques will be evaluated. The results show that it is possible to have an accuracy of up to 97.50% for the classification of contaminated insulators from the extraction of characteristics with computer vision using the NN for the classification. The proposed model is more accurate than well-established models such as support-vector machine (SVM), k-nearest neighbor (k-NN), and ensemble learning methods. This showed that optimizing the model's parameters can make it superior to solve the problem in question."
Review and classification of AI-enabled COVID-19 CT imaging models based on computer vision tasks,"This article presents a systematic overview of artificial intelligence (AI) and computer vision strategies for diagnosing the coronavirus disease of 2019 (COVID-19) using computerized tomography (CT) medical images. We analyzed the previous review works and found that all of them ignored classifying and categorizing COVID-19 literature based on computer vision tasks, such as classification, segmentation, and detection. Most of the COVID-19 CT diagnosis methods comprehensively use segmentation and classification tasks. Moreover, most of the review articles are diverse and cover CT as well as X-ray images. Therefore, we focused on the COVID-19 diagnostic methods based on CT images. Well-known search engines and databases such as Google, Google Scholar, Kaggle, Baidu, IEEE Xplore, Web of Science, PubMed, ScienceDirect, and Scopus were utilized to collect relevant studies. After deep analysis, we collected 114 studies and reported highly enriched information for each selected research. According to our analysis, AI and computer vision have substantial potential for rapid COVID-19 diagnosis as they could significantly assist in automating the diagnosis process. Accurate and efficient models will have real-time clinical implications, though further research is still required. Categorization of literature based on computer vision tasks could be helpful for future research; therefore, this review article will provide a good foundation for conducting such research."
"Computer Vision Algorithms, Remote Sensing Data Fusion Techniques, and Mapping and Navigation Tools in the Industry 4.0-Based Slovak Automotive Sector","The objectives of this paper, and the novelty brought to the topic of the Industry 4.0 manufacturing systems, are related to the integration of computer vision algorithms, remote sensing data fusion techniques, and mapping and navigation tools in the Slovak automotive sector. We conducted a thorough examination of Industry 4.0-based value and supply chains, clarifying how cyber-physical production systems operate in relation to collision avoidance technologies, environment mapping algorithms, and mobility simulation tools in network connectivity systems through vehicle navigation data. The Citroen C3 and Peugeot 208 automobiles are two examples of high-tech products whose worldwide value and supply chain development trends were examined in this study by determining countries and their contributions to production. The fundamental components of the research—statistical analysis and visual analysis—were utilized in conjunction with a variety of syntheses, comparisons, and analytical methodologies. A case study was developed using PSA Group SVK data. The graphical analysis revealed that Slovakia offers the second-highest added value to the chosen items, but it also highlighted the country’s slow-growing research and development (R&D) infrastructure, which could lead to a subsequent loss of investment and business as usual. Slovakia can generate better export added value by optimizing Industry 4.0-based manufacturing systems in the automotive sector."
Computer Vision Techniques in Construction: A Critical Review,"Computer vision has been gaining interest in a wide range of research areas in recent years, from medical to industrial robotics. The architecture, engineering and construction and facility management sector ranks as one of the most intensive fields where vision-based systems/methods are used to facilitate decision making processes during the construction phase. Construction sites make efficient monitoring extremely tedious and difficult due to clutter and disorder. Extensive research has been carried out to investigate the potential to utilise computer vision for assisting on-site managerial tasks. This paper reviews studies on computer vision in the past decade, with a focus on state-of-the-art methods in a typical vision-based scheme, and discusses challenges associated with their application. This research aims to guide practitioners to successfully find suitable approaches for a particular project."
Deep learning-enabled medical computer vision,"A decade of unprecedented progress in artificial intelligence (AI) has demonstrated the potential for many fields—including medicine—to benefit from the insights that AI techniques can extract from data. Here we survey recent progress in the development of modern computer vision techniques—powered by deep learning—for medical applications, focusing on medical imaging, medical video, and clinical deployment. We start by briefly summarizing a decade of progress in convolutional neural networks, including the vision tasks they enable, in the context of healthcare. Next, we discuss several example medical imaging applications that stand to benefit—including cardiology, pathology, dermatology, ophthalmology–and propose new avenues for continued work. We then expand into general medical video, highlighting ways in which clinical workflows can integrate computer vision to enhance care. Finally, we discuss the challenges and hurdles required for real-world clinical deployment of these technologies."
Do Datasets Have Politics? Disciplinary Values in Computer Vision Dataset Development,"Data is a crucial component of machine learning. The field is reliant on data to train, validate, and test models. With increased technical capabilities, machine learning research has boomed in both academic and industry settings, and one major focus has been on computer vision. Computer vision is a popular domain of machine learning increasingly pertinent to real-world applications, from facial recognition in policing to object detection for autonomous vehicles. Given computer vision's propensity to shape machine learning research and impact human life, we seek to understand disciplinary practices around dataset documentation - how data is collected, curated, annotated, and packaged into datasets for computer vision researchers and practitioners to use for model tuning and development. Specifically, we examine what dataset documentation communicates about the underlying values of vision data and the larger practices and goals of computer vision as a field. To conduct this study, we collected a corpus of about 500 computer vision datasets, from which we sampled 114 dataset publications across different vision tasks. Through both a structured and thematic content analysis, we document a number of values around accepted data practices, what makes desirable data, and the treatment of humans in the dataset construction process. We discuss how computer vision datasets authors value efficiency at the expense of care; universality at the expense of contextuality; impartiality at the expense of positionality; and model work at the expense of data work. Many of the silenced values we identify sit in opposition with social computing practices. We conclude with suggestions on how to better incorporate silenced values into the dataset creation and curation process."
Deep Learning vs. Traditional Computer Vision,"Deep Learning has pushed the limits of what was possible in the domain of Digital Image Processing. However, that is not to say that the traditional computer vision techniques which had been undergoing progressive development in years prior to the rise of DL have become obsolete. This paper will analyse the benefits and drawbacks of each approach. The aim of this paper is to promote a discussion on whether knowledge of classical computer vision techniques should be maintained. The paper will also explore how the two sides of computer vision can be combined. Several recent hybrid methodologies are reviewed which have demonstrated the ability to improve computer vision performance and to tackle problems not suited to Deep Learning. For example, combining traditional computer vision techniques with Deep Learning has been popular in emerging domains such as Panoramic Vision and 3D vision for which Deep Learning models have not yet been fully optimised."
Florence: A New Foundation Model for Computer Vision,"Automated visual understanding of our diverse and open world demands computer vision models to generalize well with minimal customization for specific tasks, similar to human vision. Computer vision foundation models, which are trained on diverse, large-scale dataset and can be adapted to a wide range of downstream tasks, are critical for this mission to solve real-world computer vision applications. While existing vision foundation models such as CLIP, ALIGN, and Wu Dao 2.0 focus mainly on mapping images and textual representations to a cross-modal shared representation, we introduce a new computer vision foundation model, Florence, to expand the representations from coarse (scene) to fine (object), from static (images) to dynamic (videos), and from RGB to multiple modalities (caption, depth). By incorporating universal visual-language representations from Web-scale image-text data, our Florence model can be easily adapted for various computer vision tasks, such as classification, retrieval, object detection, VQA, image caption, video retrieval and action recognition. Moreover, Florence demonstrates outstanding performance in many types of transfer learning: fully sampled fine-tuning, linear probing, few-shot transfer and zero-shot transfer for novel images and objects. All of these properties are critical for our vision foundation model to serve general purpose vision tasks. Florence achieves new state-of-the-art results in majority of 44 representative benchmarks, e.g., ImageNet-1K zero-shot classification with top-1 accuracy of 83.74 and the top-5 accuracy of 97.18, 62.4 mAP on COCO fine tuning, 80.36 on VQA, and 87.8 on Kinetics-600."
Learning To Resize Images for Computer Vision Tasks,"For all the ways convolutional neural nets have revolutionized computer vision in recent years, one important aspect has received surprisingly little attention: the effect of image size on the accuracy of tasks being trained for. Typically, to be efficient, the input images are resized to a relatively small spatial resolution (e.g. 224x224), and both training and inference are carried out at this resolution. The actual mechanism for this re-scaling has been an afterthought: Namely, off-the-shelf image resizers such as bilinear and bicubic are commonly used in most machine learning software frameworks. But do these resizers limit the on task performance of the trained networks? The answer is yes. Indeed, we show that the typical linear resizer can be replaced with learned resizers that can substantially improve performance. Importantly, while the classical resizers typically result in better perceptual quality of the downscaled images, our proposed learned resizers do not necessarily give better visual quality, but instead improve task performance. Our learned image resizer is jointly trained with a baseline vision model. This learned CNN-based resizer creates machine friendly visual manipulations that lead to a consistent improvement of the end task metric over the baseline model. Specifically, here we focus on the classification task with the ImageNet dataset, and experiment with four different models to learn resizers adapted to each model. Moreover, we show that the proposed resizer can also be useful for fine-tuning the classification baselines for other vision tasks. To this end, we experiment with three different baselines to develop image quality assessment (IQA) models on the AVA dataset."
A critical review on computer vision and artificial intelligence in food industry,"Emerging technologies such as computer vision and Artificial Intelligence (AI) are estimated to leverage the accessibility of big data for active training and yielding operational real time smart machines and predictable models. This phenomenon of applying vision and learning methods for the improvement of food industry is termed as computer vision and AI driven food industry. This review contributes to provide an insight into state-of-the-art AI and computer vision technologies that can assist farmers in agriculture and food processing. This paper investigates various scenarios and use cases of machine learning, machine vision and deep learning in global perspective with the lens of sustainability. It explains the increasing demand towards the AgTech industry using computer vision and AI which might be a path towards sustainable food production to feed the future. Also, this review tosses some implications regarding challenges and recommendations in inclusion of technologies in real time farming, substantial global policies and investments. Finally, the paper discusses the possibility of using Fourth Industrial Revolution [4.0 IR] technologies such as deep learning and computer vision robotics as a key for sustainable food production."
A review of computer vision–based structural health monitoring at local and global levels,"Structural health monitoring at local and global levels using computer vision technologies has gained much attention in the structural health monitoring community in research and practice. Due to the computer vision technology application advantages such as non-contact, long distance, rapid, low cost and labor, and low interference to the daily operation of structures, it is promising to consider computer vision–structural health monitoring as a complement to the conventional structural health monitoring. This article presents a general overview of the concepts, approaches, and real-life practice of computer vision–structural health monitoring along with some relevant literature that is rapidly accumulating. The computer vision–structural health monitoring covered in this article at local level includes applications such as crack, spalling, delamination, rust, and loose bolt detection. At the global level, applications include displacement measurement, structural behavior analysis, vibration serviceability, modal identification, model updating, damage detection, cable force monitoring, load factor estimation, and structural identification using input–output information. The current research studies and applications of computer vision–structural health monitoring mainly focus on the implementation and integration of two-dimensional computer vision techniques to solve structural health monitoring problems and the projective geometry methods implemented are utilized to convert the three-dimensional problems into two-dimensional problems. This review mainly puts emphasis on two-dimensional computer vision–structural health monitoring applications. Subsequently, a brief review of representative developments of three-dimensional computer vision in the area of civil engineering is presented along with the challenges and opportunities of two-dimensional and three-dimensional computer vision–structural health monitoring. Finally, the article presents a forward look to the future of computer vision–structural health monitoring."
Computer vision algorithms and hardware implementations: A survey,"The field of computer vision is experiencing a great-leap-forward development today. This paper aims at providing a comprehensive survey of the recent progress on computer vision algorithms and their corresponding hardware implementations. In particular, the prominent achievements in computer vision tasks such as image classification, object detection and image segmentation brought by deep learning techniques are highlighted. On the other hand, review of techniques for implementing and optimizing deep-learning-based computer vision algorithms on GPU, FPGA and other new generations of hardware accelerators are presented to facilitate real-time and/or energy-efficient operations. Finally, several promising directions for future research are presented to motivate further development in the field."
Computer vision technology in agricultural automation —A review,"Computer vision is a field that involves making a machine “see”. This technology uses a camera and computer instead of the human eye to identify, track and measure targets for further image processing. With the development of computer vision, such technology has been widely used in the field of agricultural automation and plays a key role in its development. This review systematically summarizes and analyzes the technologies and challenges over the past three years and explores future opportunities and prospects to form the latest reference for researchers. Through the analyses, it is found that the existing technology can help the development of agricultural automation for small field farming to achieve the advantages of low cost, high efficiency and high precision. However, there are still major challenges. First, the technology will continue to expand into new application areas in the future, and there will be more technological issues that need to be overcome. It is essential to build large-scale data sets. Second, with the rapid development of agricultural automation, the demand for professionals will continue to grow. Finally, the robust performance of related technologies in various complex environments will also face challenges. Through analysis and discussion, we believe that in the future, computer vision technology will be combined with intelligent technology such as deep learning technology, be applied to every aspect of agricultural production management based on large-scale datasets, be more widely used to solve the current agricultural problems, and better improve the economic, general and robust performance of agricultural automation systems, thus promoting the development of agricultural automation equipment and systems in a more intelligent direction."
Kornia: an Open Source Differentiable Computer Vision Library for PyTorch,"This work presents Kornia -- an open source computer vision library which consists of a set of differentiable routines and modules to solve generic computer vision problems. At its core, the package uses PyTorch as its main backend both for efficiency and to take advantage of the reverse-mode auto-differentiation to define and compute the gradient of complex functions. Inspired by OpenCV, Kornia is composed of a set of modules containing operators that can be inserted inside neural networks to train models to perform image transformations, camera calibration, epipolar geometry, and low level image processing techniques such as filtering and edge detection that operate directly on high dimensional tensor representations. Examples of classical vision problems implemented using our framework are also provided including a benchmark comparing to existing vision libraries."
A Fourier Perspective on Model Robustness in Computer Vision,"Achieving robustness to distributional shift is a longstanding and challenging goal of computer vision. Data augmentation is a commonly used approach for improving robustness, however robustness gains are typically not uniform across corruption types. Indeed increasing performance in the presence of random noise is often met with reduced performance on other corruptions such as contrast change. Understanding when and why these sorts of trade-offs occur is a crucial step towards mitigating them. Towards this end, we investigate recently observed trade-offs caused by Gaussian data augmentation and adversarial training. We find that both methods improve robustness to corruptions that are concentrated in the high frequency domain while reducing robustness to corruptions that are concentrated in the low frequency domain. This suggests that one way to mitigate these trade-offs via data augmentation is to use a more diverse set of augmentations. Towards this end we observe that AutoAugment, a recently proposed data augmentation policy optimized for clean accuracy, achieves state-of-the-art robustness on the CIFAR-10-C benchmark."
Events-To-Video: Bringing Modern Computer Vision to Event Cameras,"Event cameras are novel sensors that report brightness changes in the form of asynchronous ""events"" instead of intensity frames. They have significant advantages over conventional cameras: high temporal resolution, high dynamic range, and no motion blur. Since the output of event cameras is fundamentally different from conventional cameras, it is commonly accepted that they require the development of specialized algorithms to accommodate the particular nature of events. In this work, we take a different view and propose to apply existing, mature computer vision techniques to videos reconstructed from event data. We propose a novel, recurrent neural network to reconstruct videos from a stream of events and train it on a large amount of simulated event data. Our experiments show that our approach surpasses state-of-the-art reconstruction methods by a large margin (> 20%) in terms of image quality. We further apply off-the-shelf computer vision algorithms to videos reconstructed from event data on tasks such as object classification and visual-inertial odometry, and show that this strategy consistently outperforms algorithms that were specifically designed for event data. We believe that our approach opens the door to bringing the outstanding properties of event cameras to an entirely new range of tasks."
Computer Vision For COVID-19 Control: A Survey,"The COVID-19 pandemic has triggered an urgent need to contribute to the fight against an immense threat to the human population. Computer Vision, as a subfield of Artificial Intelligence, has enjoyed recent success in solving various complex problems in health care and has the potential to contribute to the fight of controlling COVID-19. In response to this call, computer vision researchers are putting their knowledge base at work to devise effective ways to counter COVID-19 challenge and serve the global community. New contributions are being shared with every passing day. It motivated us to review the recent work, collect information about available research resources and an indication of future research directions. We want to make it available to computer vision researchers to save precious time. This survey paper is intended to provide a preliminary review of the available literature on the computer vision efforts against COVID-19 pandemic."
Hand Gesture Recognition Based on Computer Vision: A Review of Techniques,"Hand gestures are a form of nonverbal communication that can be used in several fields such as communication between deaf-mute people, robot control, human–computer interaction (HCI), home automation and medical applications. Research papers based on hand gestures have adopted many different techniques, including those based on instrumented sensor technology and computer vision. In other words, the hand sign can be classified under many headings, such as posture and gesture, as well as dynamic and static, or a hybrid of the two. This paper focuses on a review of the literature on hand gesture techniques and introduces their merits and limitations under different circumstances. In addition, it tabulates the performance of these methods, focusing on computer vision techniques that deal with the similarity and difference points, technique of hand segmentation used, classification algorithms and drawbacks, number and types of gestures, dataset used, detection range (distance) and type of camera used. This paper is a thorough general overview of hand gesture methods with a brief discussion of some possible applications."
Deep learning and computer vision will transform entomology,"Most animal species on Earth are insects, and recent reports suggest that their abundance is in drastic decline. Although these reports come from a wide range of insect taxa and regions, the evidence to assess the extent of the phenomenon is sparse. Insect populations are challenging to study, and most monitoring methods are labor intensive and inefficient. Advances in computer vision and deep learning provide potential new solutions to this global challenge. Cameras and other sensors can effectively, continuously, and noninvasively perform entomological observations throughout diurnal and seasonal cycles. The physical appearance of specimens can also be captured by automated imaging in the laboratory. When trained on these data, deep learning models can provide estimates of insect abundance, biomass, and diversity. Further, deep learning models can quantify variation in phenotypic traits, behavior, and interactions. Here, we connect recent developments in deep learning and computer vision to the urgent demand for more cost-efficient monitoring of insects and other invertebrates. We present examples of sensor-based monitoring of insects. We show how deep learning tools can be applied to exceptionally large datasets to derive ecological information and discuss the challenges that lie ahead for the implementation of such solutions in entomology. We identify four focal areas, which will facilitate this transformation: 1) validation of image-based taxonomic identification; 2) generation of sufficient training data; 3) development of public, curated reference databases; and 4) solutions to integrate deep learning and molecular tools."
Computer vision for behaviour-based safety in construction: A review and future directions,"The process of identifying and bringing to the fore people’s unsafe behaviour is a core function of implementing a behaviour-based safety (BBS) program in construction. This can be a labour-intensive and challenging process but is needed to enable people to reflect and learn about how their unsafe actions can jeopardise not only their safety but that of their co-workers. With advances being made in computer vision, the capability exists to automatically capture and identify unsafe behaviour and hazards in real-time from two-dimensional (2D) digital images/videos. The corollary developments in computer vision have stimulated a wealth of research in construction to examine its potential application to practice. Hindering the application of computer vision in construction has been its inability to accurately, and generalise the detection of objects. To address this shortcoming, developments in deep learning have provided computer vision with the ability to improve the accuracy, reliability and ability to generalise object detection and therefore its usage in construction. In this paper we review the developments of computer vision studies that have been used to identify unsafe behaviour from 2D images that arises on construction sites. Then, in light of advances made with deep learning, we examine and discuss its integration with computer vision to support BBS. We also suggest that future computer-vision research should aim to support BBS by being able to: (1) observe and record unsafe behaviour; (2) understand why people act unsafe behaviour; (3) learn from unsafe behaviour; and (4) predict unsafe behaviour."
Tensor Methods in Computer Vision and Deep Learning,"Tensors, or multidimensional arrays, are data structures that can naturally represent visual data of multiple dimensions. Inherently able to efficiently capture structured, latent semantic spaces and high-order interactions, tensors have a long history of applications in a wide span of computer vision problems. With the advent of the deep learning paradigm shift in computer vision, tensors have become even more fundamental. Indeed, essential ingredients in modern deep learning architectures, such as convolutions and attention mechanisms, can readily be considered as tensor mappings. In effect, tensor methods are increasingly finding significant applications in deep learning, including the design of memory and compute efficient network architectures, improving robustness to random noise and adversarial attacks, and aiding the theoretical understanding of deep networks. This article provides an in-depth and practical review of tensors and tensor methods in the context of representation learning and deep learning, with a particular focus on visual data analysis and computer vision applications. Concretely, besides fundamental work in tensor-based visual data analysis methods, we focus on recent developments that have brought on a gradual increase in tensor methods, especially in deep learning architectures and their implications in computer vision applications. To further enable the newcomer to grasp such concepts quickly, we provide companion Python notebooks, covering key aspects of this article and implementing them, step-by-step with TensorLy."
Generative Adversarial Networks in Computer Vision: A Survey and Taxonomy,"Generative adversarial networks (GANs) have been extensively studied in the past few years. Arguably their most significant impact has been in the area of computer vision where great advances have been made in challenges such as plausible image generation, image-to-image translation, facial attribute manipulation, and similar domains. Despite the significant successes achieved to date, applying GANs to real-world problems still poses significant challenges, three of which we focus on here. These are as follows: (1) the generation of high quality images, (2) diversity of image generation, and (3) stabilizing training. Focusing on the degree to which popular GAN technologies have made progress against these challenges, we provide a detailed review of the state-of-the-art in GAN-related research in the published scientific literature. We further structure this review through a convenient taxonomy we have adopted based on variations in GAN architectures and loss functions. While several reviews for GANs have been presented to date, none have considered the status of this field based on their progress toward addressing practical challenges relevant to computer vision. Accordingly, we review and critically discuss the most popular architecture-variant, and loss-variant GANs, for tackling these challenges. Our objective is to provide an overview as well as a critical analysis of the status of GAN research in terms of relevant progress toward critical computer vision application requirements. As we do this we also discuss the most compelling applications in computer vision in which GANs have demonstrated considerable success along with some suggestions for future research directions. Codes related to the GAN-variants studied in this work is summarized on https://github.com/sheqi/GAN_Review."
Analysis of Explainers of Black Box Deep Neural Networks for Computer Vision: A Survey,"Deep Learning is a state-of-the-art technique to make inference on extensive or complex data. As a black box model due to their multilayer nonlinear structure, Deep Neural Networks are often criticized as being non-transparent and their predictions not traceable by humans. Furthermore, the models learn from artificially generated datasets, which often do not reflect reality. By basing decision-making algorithms on Deep Neural Networks, prejudice and unfairness may be promoted unknowingly due to a lack of transparency. Hence, several so-called explanators, or explainers, have been developed. Explainers try to give insight into the inner structure of machine learning black boxes by analyzing the connection between the input and output. In this survey, we present the mechanisms and properties of explaining systems for Deep Neural Networks for Computer Vision tasks. We give a comprehensive overview about the taxonomy of related studies and compare several survey papers that deal with explainability in general. We work out the drawbacks and gaps and summarize further research ideas."
Overview: Computer Vision and Machine Learning for Microstructural Characterization and Analysis,"Microstructural characterization and analysis is the foundation of microstructural science, connecting materials structure to composition, process history, and properties. Microstructural quantification traditionally involves a human deciding what to measure and then devising a method for doing so. However, recent advances in computer vision (CV) and machine learning (ML) offer new approaches for extracting information from microstructural images. This overview surveys CV methods for numerically encoding the visual information contained in a microstructural image using either feature-based representations or convolutional neural network (CNN) layers, which then provides input to supervised or unsupervised ML algorithms that find associations and trends in the high-dimensional image representation. CV/ML systems for microstructural characterization and analysis span the taxonomy of image analysis tasks, including image classification, semantic segmentation, object detection, and instance segmentation. These tools enable new approaches to microstructural analysis, including the development of new, rich visual metrics and the discovery of processing-microstructure-property relationships."
Evaluating Scalable Bayesian Deep Learning Methods for Robust Computer Vision,"While deep neural networks have become the go-to approach in computer vision, the vast majority of these models fail to properly capture the uncertainty inherent in their predictions. Estimating this predictive uncertainty can be crucial, e.g. in automotive applications. In Bayesian deep learning, predictive uncertainty is commonly decomposed into the distinct types of aleatoric and epistemic uncertainty. The former can be estimated by letting a neural network output the parameters of a certain probability distribution. Epistemic uncertainty estimation is a more challenging problem, and while different scalable methods recently have emerged, no extensive comparison has been performed in a real-world setting. We therefore accept this task and propose a comprehensive evaluation framework for scalable epistemic uncertainty estimation methods in deep learning. Our proposed framework is specifically designed to test the robustness required in real-world computer vision applications. We also apply this framework to provide the first properly extensive and conclusive comparison of the two current state-of-the-art scalable methods: ensembling and MC-dropout. Our comparison demonstrates that ensembling consistently provides more reliable and practically useful uncertainty estimates."
A survey of public datasets for computer vision tasks in precision agriculture,"Computer vision technologies have attracted significant interest in precision agriculture in recent years. At the core of robotics and artificial intelligence, computer vision enables various tasks from planting to harvesting in the crop production cycle to be performed automatically and efficiently. However, the scarcity of public image datasets remains a crucial bottleneck for fast prototyping and evaluation of computer vision and machine learning algorithms for the targeted tasks. Since 2015, a number of image datasets have been established and made publicly available to alleviate this bottleneck. Despite this progress, a dedicated survey on these datasets is still lacking. To fill this gap, this paper makes the first comprehensive but not exhaustive review of the public image datasets collected under field conditions for facilitating precision agriculture, which include 15 datasets on weed control, 10 datasets on fruit detection, and 9 datasets on miscellaneous applications. We survey the main characteristics and applications of these datasets, and discuss the key considerations for creating high-quality public image datasets. This survey paper will be valuable for the research community on the selection of suitable image datasets for algorithm development and identification of where creation of new image datasets is needed to support precision agriculture."
Introduction to the IEEE Transactions on Cloud Computing,"Cloud computing is a new field in Internet computing that provides novel perspectives in internetworking technologies and raises issues in the architecture, design, and implementation of existing networks and data centers. The relevant research has just recently gained momentum, and the space of potential ideas and solutions is still far from being widely explored."
Service-Oriented Computing and Cloud Computing: Challenges and Opportunities,"Service-oriented computing and cloud computing have a reciprocal relationship - one provides computing of services and the other provides services of computing. Although service-oriented computing in cloud computing environments presents a new set of research challenges, the authors believe the combination also provides potentially transformative opportunities"
Cloud Computing: Opportunities and Challenges,"We live and operate in the world of computing and computers. The Internet has drastically changed the computing world from the concept of parallel computing to distributed computing to grid computing and now to cloud computing. Cloud computing is a new wave in the field of information technology. Some see it as an emerging field in computer science. It consists of a set of resources and services offered through the Internet. Hence, ?cloud computing? is also called ?Internet computing.? The word ?cloud? is a metaphor for describing the Web as a space where computing has been preinstalled and exists as a service. Operating systems, applications, storage, data, and processing capacity all exist on the Web, ready to be shared among users. Figure 1 shows a conceptual diagram of cloud computing."
Scientific Cloud Computing: Early Definition and Experience,"Cloud computing emerges as a new computing paradigm which aims to provide reliable, customized and QoS guaranteed computing dynamic environments for end-users. This paper reviews recent advances of Cloud computing, identifies the concepts and characters of scientific Clouds, and finally presents an example of scientific Cloud for data centers"
Cloud Computing: IT as a Service,"Cloud computing has been a dominant IT news topic over the past year, yet what it is and its industry impact—realized and potential—are only gradually becoming clear. To help demystify the matter, the authors presented a panel at the 2008 IEEE International Conference on Web Services (ICWS), held last September in Beijing. The panel, ""Cloud Computing and IT as a Service: Opportunities and Challenges,"" featured presentations from three IT vendor giants—Microsoft, IBM, and Cisco—addressing industry perspectives and cloud computing initiatives. This installment of the Trends department distills the authors' panel session and provides updates in the year since it occurred."
"
Cloud computing - concepts, architecture and challenges","With the advent internet in the 1990s to the present day facilities of ubiquitous computing, the internet has changed the computing world in a drastic way. It has traveled from the concept of parallel computing to distributed computing to grid computing and recently to cloud computing. Although the idea of cloud computing has been around for quite some time, it is an emerging field of computer science. Cloud computing can be defined as a computing environment where computing needs by one party can be outsourced to another party and when need be arise to use the computing power or resources like database or emails, they can access them via internet. Cloud computing is a recent trend in IT that moves computing and data away from desktop and portable PCs into large data centers. The main advantage of cloud computing is that customers do not have to pay for infrastructure, its installation, required man power to handle such infrastructure and maintenance. In this paper we will discuss what makes all this possible, what is the architectural design of cloud computing and its applications."
Cloud Computing,"Cloud computing is no longer on the horizon; it has become the next logical step for the IT industry. It's the new strategic weapon in enterprise computing and the new norm in every sector of society. Businesses, educational institutions, governments, community organizations, and individuals are looking at cloud offerings to manage information instead of infrastructure. In this issue, the theme articles discuss migration performance, energy-efficiency enhancement, and security issues in cloud computing."
Hot Topics in Cloud Computing,"Cloud computing is no longer just hype. It's quickly evolving and gradually realizing its business value, as the articles in this special issue show. It's now attracting more and more researchers and practitioners, who are creating innovations around its core enabling technologies and architectural building blocks."
Towards Cloud Computing: A Literature Review on Cloud Computing and Its Development Trends,"This article contains a review of technical literature relating the definitions, characteristics, operations, security management, service governance, and development trends of cloud computing. The advantages and disadvantages of cloud computing are respectively described to represent the impacts of cloud computing in various fields. The article also explains the structure of clouds in service-oriented architectures and summarizes six major development trends, which can be utilized as a reference for entrepreneurs and researchers"
Cloud Computing: Distributed Internet Computing for IT and Scientific Research,"Cloud computing is a disruptive technology with profound implications not only for Internet services but also for the IT sector as a whole. Its emergence promises to streamline the on-demand provisioning of software, hardware, and data as a service, achieving economies of scale in IT solutions' deployment and operation. This issue's articles tackle topics including architecture and management of cloud computing infrastructures, SaaS and IaaS applications, discovery of services and data in cloud computing infrastructures, and cross-platform interoperability. Still, several outstanding issues exist, particularly related to SLAs, security and privacy, and power efficiency. Other open issues include ownership, data transfer bottlenecks, performance unpredictability, reliability, and software licensing issues. Finally, hosted applications' business models must show a clear pathway to monetizing cloud computing. Several companies have already built Internet consumer services such as search, social networking, Web email, and online commerce that use cloud computing infrastructure. Above all, cloud computing's still unknown ""killer application"" will determine many of the challenges and the solutions we must develop to make this technology work in practice"
Challenges of Connecting Edge and Cloud Computing: A Security and Forensic Perspective,"A  key benefit of connecting edge and cloud computing is the capability to achieve high-throughput under high concurrent accesses, mobility support, real-time processing guarantees, and data persistency. For example, the elastic provisioning and storage capabilities provided by cloud computing allow us to cope with scalability, persistency and reliability requirements and to adapt the infrastructure capacity to the exacting needs based on the amount of generated data."
Cloud Computing: New Wine or Just a New Bottle?,"Cloud computing has evolved from previous computing paradigms going back as far to the days of mainframes, but is it really different? Do the explosive new capabilities from cloud computing solve any of the problems left unsolved from three decades ago? The authors in this issue discuss their views on what cloud computing is leaving the reader to decide for themselves."
Security and Privacy-Preserving Challenges of e-Health Solutions in Cloud Computing,"A systematic and comprehensive review of security and privacy-preserving challenges in e-health solutions indicates various privacy preserving approaches to ensure privacy and security of electronic health records (EHRs) in the cloud. This paper highlights the research challenges and directions concerning cyber security to build a comprehensive security model for EHR. We carry an intensive study in the IEEE, Science Direct, Google Scholar, PubMed, and ACM for papers on EHR approach published between 2000 and 2018 and summarized them in terms of the architecture types as well as evaluation strategies. We surveyed, investigated, and reviewed various aspects of several articles and identified the following tasks: 1) EHR security and privacy; 2) security and privacy requirements of e-health data in the cloud; 3) EHR cloud architecture, and; 4) diverse EHR cryptographic and non-cryptographic approaches. We also discuss some crucial issues and the ample opportunities for advanced research related to security and privacy of EHRs. Since big data provide a great mine of information and knowledge in e-Health applications, serious privacy and security challenges that require immediate attention exist. Studies must focus on efficient comprehensive security mechanisms for EHR and also explore techniques to maintain the integrity and confidentiality of patients' information"
A Survey of Mobile Cloud Computing," Mobile Cloud Computing (MCC) is emerging as one of the most important branches of cloud computing. In this paper, MCC
is defined as cloud computing extended by mobility, and a new ad-hoc infrastructure based on mobile devices. It provides mobile
users with data storage and processing services on a cloud computing platform. Because mobile cloud computing is still in its infancy,
we aim to clarify confusion that has arisen from different views. Existing works are reviewed, and an overview of recent advances in
mobile cloud computing is provided. We investigate representative infrastructures of mobile cloud computing and analyze key
components. Moreover, emerging MCC models and services are discussed, and challenging issues are identified that will need to be
addressed in future work."
Key Challenges in Cloud Computing: Enabling the Future Internet of Services,"Cloud computing will play a major role in the future Internet of Services, enabling on-demand provisioning of applications, platforms, and computing infrastructures. However, the cloud community must address several technology challenges to turn this vision into reality. Specific issues relate to deploying future infrastructure-as-a-service clouds and include efficiently managing such clouds to deliver scalable and elastic service platforms on demand, developing cloud aggregation architectures and technologies that let cloud providers collaborate and interoperate, and improving cloud infrastructures' security, reliability, and energy efficiency."
Understanding Cloud Computing Vulnerabilities,"The current discourse about cloud computing security issues makes a well-founded assessment of cloud computing's security impact difficult for two primary reasons. First, as is true for many discussions about risk, basic vocabulary such as ""risk,"" ""threat,"" and ""vulnerability"" are often used as if they were interchangeable, without regard to their respective definitions. Second, not every issue that's raised is really specific to cloud computing. We can achieve an accurate understanding of the security issue ""delta"" that cloud computing really adds by analyzing how cloud computing influences each risk factor. One important factor concerns vulnerabilities: cloud computing makes certain well-understood vulnerabilities more significant and adds new vulnerabilities. Here, the authors define four indicators of cloud-specific vulnerabilities, introduce a security-specific cloud reference architecture, and provide examples of cloud-specific vulnerabilities for each architectural component"
Trusted Cloud Computing with Secure Resources and Data Coloring,"Trust and security have prevented businesses from fully accepting cloud platforms. To protect clouds, providers must first secure virtualized data center resources, uphold user privacy, and preserve data integrity. The authors suggest using a trust-overlay network over multiple data centers to implement a reputation system for establishing trust between service providers and data owners. Data coloring and software watermarking techniques protect shared data objects and massively distributed software modules. These techniques safeguard multi-way authentications, enable single sign-on in the cloud, and tighten access control for sensitive data in both public and private clouds."
Green house by using IOT and cloud computing,"Thereare many techniques available for the precision agriculture to monitor and control, environment for the growth of many crops. Due to unequal distribution of rain water, it is very difficult to requirement needed farmer to manage the water equally to all the crops in whole farm it requires some irrigation method that suitable for any weather condition, soil types and variety of crops. Green House is the best solution to control and mange all this problem It is more important to search a method that gives perfect analyzation and controlling to develop proper environment. Large areas covered by sensor network this can establish greenhouse with precision environment required for different crops. This environment builds up by using two technologies it and cloud computing. By using IOT(Internet on things) we control devices or any environmental needs anytime, anywhere and the cloud which provides storage and computing resources to implement a web page."
Cloud Computing: Issues and Challenges,"Many believe that Cloud will reshape the entire ICT industry as a revolution. In this paper, we aim to pinpoint the challenges and issues of Cloud computing. We first discuss two related computing paradigms - Service-Oriented Computing and Grid computing, and their relationships with Cloud computing. We then identify several challenges from the Cloud computing adoption perspective. Last, we will highlight the Cloud interoperability issue that deserves substantial further research and development."
NIST Cloud Computing Reference Architecture,"This paper presents the first version of the NIST Cloud Computing Reference Architecture (RA). This is a vendor neutral conceptual model that concentrates on the role and interactions of the identified actors in the cloud computing sphere. Five primary actors were identified - Cloud Service Consumer, Cloud Service Provider, Cloud Broker, Cloud Auditor and Cloud Carrier. Their roles and activities are discussed in this report. A primary goal for generating this model was to give the United States Government (USG) a method for understanding and communicating the components of a cloud computing system for Federal IT executives, Program Managers and IT procurement officials."
Cloud Computing: The Fifth Generation of Computing,"Cloud Computing is the emerging buzzword in Information Technology. It is growing day by day due to its rich features of services. It is a virtual pool of resources which are provided to the users through Internet. Cloud computing is a new flavor of computing where our trend of using Internet changes. It is the future of Internet. It is also called as fifth generation of computing after Mainframe, Personal Computer, Client-Server Computing, and the Web. Nowadays, various Internet services are available in distributed manner. To use these services in a feasible manner is a big question because sometimes many resources become idle, they are costly and increase the budget of organization. This is the great matter of concern, especially when the world is facing financial crisis. Cloud Computing can be the answer of these questions. In this paper, we have analyzed and highlighted the various aspects of Cloud Computing to find the actuality of the fifth generation computing in the form of cloud computing."
NCSU's Virtual Computing Lab: A Cloud Computing Solution,"The delivery of many diverse computing services over the Internet, with flexible provisioning, has led to much greater efficiency, substantial cost savings, and many ways to enable and empower end users. NCSU's own experience with cloud computing, through its Virtual Computing Lab, indicates that this approach would be beneficial to a much wider audience.ays to enable and empower end users. NCSU's own experience with cloud computing, through its Virtual Computing Lab, indicates that this approach would be beneficial to a much wider audience."
Recent Trends in Energy-Efficient Cloud Computing,"Almost every online user directly or indirectly uses cloud computing, which is the most promising information and communication technology (ICT) paradigm. However, cloud computing's ultrascale size requires large datacenters comprising thousands of servers and other supporting equipment. The power consumption share of such infrastructures reaches 1.1 percent to 1.5 percent of the total electricity use worldwide, and is projected to rise even more. In this article, the authors describe recent trends in cloud computing regarding the energy efficiency of its supporting infrastructure. They present state-of-the-art approaches found in literature and in practice covering servers, networking, cloud management systems, and appliances (user software). They also describe benefits and trade-offs when applying energy-efficiency techniques, and discuss existing challenges and future research directions."
A Multiple QoS Constrained Scheduling Strategy of Multiple Workflows for Cloud Computing,"Cloud computing has gained popularity in recent times. As a cloud must provide services to many users at the same time and different users have different QoS requirements, the scheduling strategy should be developed for multiple workflows with different QoS requirements. In this paper, we introduce a Multiple QoS Constrained Scheduling Strategy of Multi-Workflows (MQMW) to address this problem. The strategy can schedule multiple workflows which are started at any time and the QoS requirements are taken into account. Experimentation shows that our strategy is able to increase the scheduling success rate significantly."
Cloud Computing for Emerging Mobile Cloud Apps,"The tutorial will begin with an explanation of the concepts behind cloud computing systems, cloud software architecture, the need for mobile cloud computing as an aspect of the app industry to deal with new mobile app design, network apps, app designing tools, and the motivation for migrating apps to cloud computing systems. The tutorial will review facts, goals and common architectures of mobile cloud computing systems, as well as introduce general mobile cloud services for app developers and marketers. This tutorial will highlight some of the major challenges and costs, and the role of mobile cloud computing architecture in the field of app design, as well as how the app-design industry has an opportunity to migrate to cloud computing systems with low investment. The tutorial will review privacy and security issues. It will describe major mobile cloud vendor services to illustrate how mobile cloud vendors can improve mobile app businesses. We will consider major cloud vendors, such as Microsoft Windows Azure, Amazon AWS and Google Cloud Platform. Finally, the tutorial will survey some of the cuttingedge practices in the field, and present some opportunities for future development."
Toward Vehicle-Assisted Cloud Computing for Smartphones,"Mobile cloud computing is an emerging technology for facilitating complex application execution on smartphones. Cloud services are utilized not only to speed up the running of mobile applications but to save energy for smartphones as well. In this paper, we propose to combine the vehicular cloud with the infrastructure-based cloud to expand the current available resources for task requests from smartphones. In our proposed architecture, the vehicular cloud acts as a cloud service provider for smartphones. Moreover, we propose a flexible offloading strategy (FOS) to carry out task migration. The vehicular cloud is able to discover and utilize the underutilized resources in vehicles to accomplish application offloading for smartphones. The FOS estimates the efficiency of various cloud service providers based on current resource conditions and then selects the suitable cloud service provider to perform the requested task. Experimental results show that the proposed approach can improve the performance of mobile applications on smartphones in terms of task response time and energy consumption."
An Efficient and Secure Dynamic Auditing Protocol for Data Storage in Cloud Computing,"In cloud computing, data owners host their data on cloud servers and users (data consumers) can access the data from cloud servers. Due to the data outsourcing, however, this new paradigm of data hosting service also introduces new security challenges, which requires an independent auditing service to check the data integrity in the cloud. Some existing remote integrity checking methods can only serve for static archive data and, thus, cannot be applied to the auditing service since the data in the cloud can be dynamically updated. Thus, an efficient and secure dynamic auditing protocol is desired to convince data owners that the data are correctly stored in the cloud. In this paper, we first design an auditing framework for cloud storage systems and propose an efficient and privacy-preserving auditing protocol. Then, we extend our auditing protocol to support the data dynamic operations, which is efficient and provably secure in the random oracle model. We further extend our auditing protocol to support batch auditing for both multiple owners and multiple clouds, without using any trusted organizer. The analysis and simulation results show that our proposed auditing protocols are secure and efficient, especially it reduce the computation cost of the auditor."
Establishing Trust in Cloud Computing,"The paper discussed the emerging technologies that can help address the challenges of trust in cloud computing. Cloud computing provides many opportunities for enterprises by offering a range of computing services. In today's competitive environment, the service dynamism, elasticity, and choices offered by this highly scalable technology are too attractive for enterprises to ignore. These opportunities, however, don't come without challenges. Cloud computing has opened up a new frontier of challenges by introducing a different type of trust scenario. Today, the problem of trusting cloud computing is a paramount concern for most enterprises. It's not that the enterprises don't trust the cloud providers' intentions; rather, they question cloud computing's capabilities."
A Scientometric Analysis of Cloud Computing Literature,"The popularity and rapid development of cloud computing in recent years has led to a huge amount of publications containing the achieved knowledge of this area of research. Due to the interdisciplinary nature and high relevance of cloud computing research, it becomes increasingly difficult or even impossible to understand the overall structure and development of this field without analytical approaches. While evaluating science has a long tradition in many fields, we identify a lack of a comprehensive scientometric study in the area of cloud computing. Based on a large bibliographic data base, this study applies scientometric means to empirically study the evolution and state of cloud computing research with a view from above the clouds. By this, we provide extensive insights into publication patterns, research impact and research productivity. Furthermore, we explore the interplay of related subtopics by analyzing keyword clusters. The results of this study provide a better understanding of patterns, trends and other important factors as a basis for directing research activities, sharing knowledge and collaborating in the area of cloud computing research."
Adaptive Fault Tolerance in Real Time Cloud Computing,"With the increasing demand and benefits of cloud computing infrastructure, real time computing can be performed on cloud infrastructure. A real time system can take advantage of intensive computing capabilities and scalable virtualized environment of cloud computing to execute real time tasks. In most of the real time cloud applications, processing is done on remote cloud computing nodes. So there are more chances of errors, due to the undetermined latency and loose control over computing node. On the other side, most of the real time systems are also safety critical and should be highly reliable. So there is an increased requirement for fault tolerance to achieve reliability for the real time computing on cloud infrastructure. In this paper, a fault tolerance model for real time cloud computing is proposed. In the proposed model, the system tolerates the faults and makes the decision on the basis of reliability of the processing nodes, i.e. virtual machines. The reliability of the virtual machines is adaptive, which changes after every computing cycle. If a virtual machine manages to produce a correct result within the time limit, its reliability increases. And if it fails to produce the result within time or correct result, its reliability decreases. A metric model is given for the reliability assessment. In the model, decrease in reliability is more than increase. If the node continues to fail, it is removed, and a new node is added. There is also a minimum reliability level. If any processing node does not achieve that level, the systems will perform backward recovery or safety measures. The proposed technique is based on the execution of design diverse variants on multiple virtual machines, and assigning reliability to the results produced by variants. The virtual machine instances can be of same type or of different types. The system provides both the forward and backward recovery mechanism, but main focus is on forward recovery. The main essence of the proposed technique is the adaptive behavior of the reliability weights assigned to each processing node and adding and removing of nodes on the basis of reliability."
Multi-document arabic text summarisation,"In this paper we present our generic extractive Arabic and English multi-document summarisers. We also describe the use of machine translation for evaluating the generated Arabic multi-document summaries using English extractive gold standards. In this work we first address the lack of Arabic multi-document corpora for summarisation and the absence of automatic and manual Arabic gold-standard summaries. These are required to evaluate any automatic Arabic summarisers. Second, we demonstrate the use of Google Translate in creating an Arabic version of the DUC-2002 dataset. The parallel Arabic/English dataset is summarised using the Arabic and English summarisation systems. The automatically generated summaries are evaluated using the ROUGE metric, as well as precision and recall. The results we achieve are compared with the top five systems in the DUC-2002 multi-document summarisation task."
Are extractive text summarisation techniques portable to broadcast news?,"In this paper we report on a series of experiments which compare the effect of individual features on both text and speech summarisation, the effect of basing the speech summaries on automatic speech recognition transcripts with varying word error rates, and the effect of summarisation approach and transcript source on summary quality. We show that classical text summarisation features (based on stylistic and content information) are portable to broadcast news. However, the quality of the speech transcripts as well as the difference in information structure between broadcast and newspaper news affect the usability of the individual features."
Extractive text summarisation in hindi,"With immense amount of data growing on web in Hindi, a text summariser would be helpful in summarising Government data, medical reports, news, and research articles. Hindi is the fourth most-spoken first language in the world. Hindi written in the Devanagari script is the official language of the Government of India. There is no public dataset for extractive summarisation available in Hindi and thus a dataset of 24253 News articles was extracted and the extractive summaries results were evaluated on various parameters with manual gold summaries of exactly 60 words each"
Arabic Topic Detection using automatic text summarisation,"With the exponential growth of the online available Arabic documents, classifying and processing large Arabic corpora has became a challenging task. The presence of noisy information embedded in these documents has made it even more difficult to get accurate results when applying a Topic Detection (TD) process. To address this problem, a proper features selection approach is needed to enhance the topic detection accuracy. In this paper, we explore the impact of using automatic summarisation technique along with a feature-selection process to enhance Arabic Topic Detection. In our work we show that using automatic summarisation reduces noisy information and results in a significant enhancement to the topic detection process and therefore increases the performance of our TD system. This was achieved by the ability of our summariser system in reducing documents size to speed up the detection process."
Video to Text Summarisation and Timestamp Generation to Detect Important Events,"With the advent of modern technology and the subsequent rise of efficient storage devices we are witnessing a rise in the number of media that is available to us. Among the most common media, the only one that takes up huge spaces on physical storage devices are videos. The primary reason for that is the addition of higher resolution videos and a greater frame rate. It is quite necessary to come up with summarisation techniques that help us understand the most important parts of the video. Apart from that, summarisation also helps us skip the non-essential parts of the video. This technology can be utilised to cut short on the time wasted on searching through the most relevant parts of the video. This paper tries to focus on the fundamental problem of summarising long videos and converting them into shorter sections that can effectively convey the same content if one were to see the entire video. Introducing timestamps also helps the viewer in jumping to the crucial events of the video. This paper makes use of deep learning algorithms such as Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN). These serve as a means of comparing different frames and generating end results."
Generating and Evaluating Text Summarisations using Text-representing Centroids(TRC),"Short abstracts and text summarisation are gaining increasing the importance of tools for filtering out the most relevant articles due to the increased documents and information. The following article uses TRCs to support text summaries’ generation and evaluation. A new method of document summarisation has been introduced based on text-representing centroids (TRC). TRC-based similarity measure delivers good similarity estimations for both methods, which are in the excellent range of 75 percent on average."
Text summarisation in progress: a literature review,"This paper contains a large literature review in the research field of Text Summarisation (TS) based on Human Language Technologies (HLT). TS helps users manage the vast amount of information available, by condensing documents’ content and extracting the most relevant facts or topics included in them. The rapid development of emerging technologies poses new challenges to this research field, which still need to be solved. Therefore, it is essential to analyse its progress over the years, and provide an overview of the past, present and future directions, highlighting the main advances achieved and outlining remaining limitations. With this purpose, several important aspects are addressed within the scope of this survey. On the one hand, the paper aims at giving a general perspective on the state-of-the-art, describing the main concepts, as well as different summarisation approaches, and relevant international forums. Furthermore, it is important to stress upon the fact that the birth of new requirements and scenarios has led to new types of summaries with specific purposes (e.g. sentiment-based summaries), and novel domains within which TS has proven to be also suitable for (e.g. blogs). In addition, TS is successfully combined with a number of intelligent systems based on HLT (e.g. information retrieval, question answering, and text classification). On the other hand, a deep study of the evaluation of summaries is also conducted in this paper, where the existing methodologies and systems are explained, as well as new research that has emerged concerning the automatic evaluation of summaries’ quality. Finally, some thoughts about TS in general and its future will encourage the reader to think of novel approaches, applications and lines to conduct research in the next years. The analysis of these issues allows the reader to have a wide and useful background on the main important aspects of this research field."
GATE: an environment to support research and development in natural language engineering,"We describe a software environment to support research and development in natural language (NL) engineering. This environment-GATE (General Architecture for Text Engineering)-aims to advance research in the area of machine processing of natural languages by providing a software infrastructure on top of which heterogeneous NL component modules may be evaluated and refined individually or may be combined into larger application systems. Thus, GATE aims to support both researchers and developers working on component technologies (e.g. parsing, tagging, morphological analysis) and those working on developing end-user applications (e.g. information extraction, text summarisation, document generation, machine translation, and second language learning). GATE will promote reuse of component technology, permit specialisation and collaboration in large-scale projects, and allow for the comparison and evaluation of alternative technologies. The first release of GATE is now available."
A Review of State-Of-The-Art Automatic Text Summarisation,"Text summarisation comes under the domain of Natural Language Processing (NLP), which entails replacing a long, precise and concise text with a shorter, precise and concise one. Manual text summarising takes a lot of time, effort and money and it's even unfeasible when there's a lot of text. Much research has been conducted since the 1950s and researchers are still developing Automatic Text Summarisation (ATS) systems. In the past few years, lots of text-summarisation algorithms and approaches have been created. In most cases, summarisation algorithms simply turn the input text into a collection of vectors or tokens. The basic objective of this research is to review the different strategies used for text summarising. There are three types of ATS approaches, namely: Extractive text summarisation approach, Abstractive text summarisation approach and Hybrid text summarisation approach. The first method chooses the relevant statements out of the given input text or document & convolves those statements to create the final output as summary. The second method converts the input document into an intermedial representation before generating a summary containing phrases that differ from the originals. Both the extractive and abstractive processes are used in the hybrid method. Despite all of the methodologies presented, the produced summaries still lag behind human-authored summaries. By addressing the various components of ATS approaches, methodologies, techniques, datasets, assessment methods and future research goals, this study provides a thorough review for researchers and novices in the field of NLP."
Multi-document-based text summarisation through deep learning algorithm,"The proposed approach is provided an effort in terms of deep leaning algorithm to retrieve an effective text summary for a set of documents. Basically, the proposed system consists of two phases such as training phase and the testing phases. The training phase is used for exploiting the three different algorithms to make the text summarisation process an effective one. Similar to every training phase, the proposed training phases is also possessed of known data and attributes. After that, the testing phase is implemented to test the efficiency of the proposed approach. For experimentation, we used four documents sets which are selected from the DUC (2002). The experimental evaluation showed expected results as, the average precision of 78%, the average recall of 1 and the average f-measure of 84%."
Exploring the efficacy and reliability of automatic text summarisation systems: Arabic texts in focus,"This study compared the salient features of the three basic types of automatic text summarisation methods (ATSMs)—extractive, abstractive, and real-time—along with the available approaches used for each type. The data set comprised 12 reports on the current issues on automatic text summarisation methods and techniques across languages, with a special focus on Arabic whose structure has been largely claimed to be problematic in most ATSMs. Three main summarizers were compared: TAAM, OTExtSum, and OntoRealSumm. Further to this, a humanoid version of the summary of the data set was prepared, and then compared to the automatically generated summary. A 10-item questionnaire was built to help with the assessment of the target ATSMs. Also, Rouge analysis was performed to assess the efficacy of all techniques in minimising the redundancy of the data set. Findings showed that the precision of the target summarizers differed considerably, as 80% of the data set has been proven to be aware of the problems underlying ATSMS. The remaining parameters were in the normal range (65–75%). In light of the equations-based assessment of ATSMS, the highest range was noted with the removal of stop word, the least range was noted with POS tagging, stem weight, and stem collection. Regarding Arabic, the statistical analysis has been proven to be the most effective summarisation method (accuracy = 57.59%; reminiscence = 58.79%; F-Value = 57.99%). Further research is required to explore how the lexicogrammatical nature of languages and generic text structure would affect the text summaristion process."
"Deep Generative Modelling: A Comparative Review of VAEs, GANs, Normalizing Flows, Energy-Based and Autoregressive Models","Deep generative models are a class of techniques that train deep neural networks to model the distribution of training samples. Research has fragmented into various interconnected approaches, each of which make trade-offs including run-time, diversity, and architectural restrictions. In particular, this compendium covers energy-based models, variational autoencoders, generative adversarial networks, autoregressive models, normalizing flows, in addition to numerous hybrid approaches. These techniques are compared and contrasted, explaining the premises behind each and how they are interrelated, while reviewing current state-of-the-art advances and implementations."
Extractive Text Summarisation using Graph Triangle Counting Approach: Proposed Method,"Currently, with a growing quantity of automated text data, the necessity for the construction of Summarisation systems turns out to be vital. Summarisation systems 
confine and condense the mainly vital ideas of the papers and assist the user to find 
and understand the foremost facts of the text quicker and easier from the dispensation 
of information. Compelling set of such systems are those that create summaries of extracts. This type of summary, which is called Extractive Summarisation , is created by 
choosing large significant fragments of the text without making any amendment to the 
original. One methodology for generating this type of summary is consuming the 
graph theory. In graph theory there is one field called graph pruning / reduction, 
which means, to find the best representation of the main graph with a smaller number 
of nodes and edges. In this paper, a graph reduction technique called the triangle 
counting approach is presented to choose the most vital sentences of the text. The first 
phase is to represent a text as a graph, where nodes are the sentences and edges are 
the similarity between the sentences. The second phase is to construct the triangles, 
after that bit vector representation and the final phase is to retrieve the sentences 
based on the values of bit vector. "
Multilingual Statistical News Summarisation: Preliminary Experiments with English,"In this paper we present a generic approach for summarising multilingual news clusters such as the ones produced by the Europe Media Monitor (EMM) system. It is generic because it uses robust statistical techniques to perform the summarisation step and its multilinguality is inherited from the multilingual entity disambiguation system used to build the source representation. We ran preliminary experiments with the TAC 2008 data, an English corpus for summarisation research, and we obtained promising improvements over a summarisation system ranked in the top 20% at the TAC 2008 competition."
New Avenues in Opinion Mining and Sentiment Analysis,"The Web holds valuable, vast, and unstructured information about public opinion. Here, the history, current use, and future of opinion mining and sentiment analysis are discussed, along with relevant techniques and tools."
Opinion Summarisation using Bi-Directional Long-Short Term Memory,"This generation, people depend on multiple resources to stay up-to-date whether be it social media or a new product available on the market. To be able to develop a model that can effectively summarise a long paragraph or a long comment posted on a website can be useful to grasp the primary information provided in that article or review hence reducing the time input to make a decision or to come to an understanding of the data. In this paper we make use of a deep learning model that helps us to get the summaries from the lengthy reviews provided by the customers. Summarisation is the method of getting a summary from a sequence of texts or files that helps us understand the basic content of the information within them. Opinion summarisation is the method of getting opinions from a set of sentences. It can be done through extractive and abstractive methods. Here we use abstractive summarisation that learns from the input dataset and comes through with thee summary with the best possible result. Our paper describes an abstractive method to get the summaries from a dataset using Bi-directional Long-Short Term Memory. We make use of an Attention layer to increase the performance of the model and help to improve the efficiency of the model. The model effectiveness is evaluated using ROUGE and BLUE scores."
Pronominal anaphora resolution using salience score for Malayalam,"Anaphora resolution (AR) is the process of resolving references to an entity in the discourse. The paper presents an algorithm to identify the pronominals and its antecedents in the Malayalam text input. Anaphora resolution is achieved by employing a hybrid of statistical machine learning and rule based approaches. The system is implemented by exploiting the morphological richness of the language and it makes use of parts of speech tagging, subject-object identification and person-number-gender of the NPs. We outline a simple, efficient but a naive algorithm for anaphora resolution, which computes the salience value score for each antecedents. The system performance is evaluated with precision, recall measures which produced promising results. The anaphora resolution system itself can improve the performance of many NLP applications such as text summarisation, text categorisation and term extraction."
A K-mixture connective-strength-based approach to automatic text summarisation,"This research focuses on developing a hybrid automatic text summarisation approach, KCS, to enhance the quality of summaries. KCS employs the K-mixture probabilistic model to establish term weight distributions in a statistical sense. It further identifies the lexical relations between nouns and nouns, as well as nouns and verbs to derive the connective strength (CS) of nouns. Sentences are ranked and extracted according to the accumulated CS values they contain. We conduct two experiments to justify the proposed approach. The results show that the K-mixture model itself is more conducive to document classification than traditional TFIDF weighting scheme since the best macro F-measure increases from 0.63 to 0.67. It, however, is still no better than the more complex linguistic-based approach that takes noun's CS into consideration. Most importantly, our proposed approach, KCS, performs best among all approaches considered (with the best macro F-measure of 0.8). It implies that KCS can extract more representative sentences from the document and its feasibility in text summarisation applications is thus justified."
An architecture for a text simplification system,"We present a pipelined architecture for a text simplification system and describe our implementation of the three stages-analysis, transformation and regeneration. Our architecture allows each component to be developed and evaluated independently. We lay particular emphasis on the discourse level aspects of syntactic simplification as these are crucial to the process and have not been dealt with by previous research in the field. These aspects include generating referring expressions, deciding determiners, deciding sentence order and preserving rhetorical and anaphoric structure."
Experimenting with Automatic Text Summarisation for Arabic,"The volume of information available on the Web is increasing rapidly. The need for systems that can automatically summarise documents is becoming ever more desirable. For this reason, text summarisation has quickly grown into a major research area as illustrated by the DUC and TAC conference series. Summarisation systems for Arabic are however still not as sophisticated and as reliable as those developed for languages like English. In this paper we discuss two summarisation systems for Arabic and report on a large user study performed on these systems. The first system, the Arabic Query-Based Text Summarisation System (AQBTSS), uses standard retrieval methods to map a query against a document collection and to create a summary. The second system, the Arabic Concept-Based Text Summarisation System (ACBTSS), creates a query-independent document summary. Five groups of users from different ages and educational levels participated in evaluating our systems."
Novel techniques for time-compressing speech: an exploratory study,"A hybrid approach for the generation of automatic text summarisation is achieved through CBS-ID3MV. A classification-based model using ID3 and multivariate (CBS-ID3MV) approach produces summaries from the text documents through classification and multiple linear regression. Efficient feature selection and extraction methods identify text features from each sentence, for the purpose of classifying summary sentences. The CBS-ID3MV model is trained with DUC 2002 training documents and the proposed approach's performance is measured at several compression rates namely 10%, 20% and 30% on the text data. The results got by the proposed framework works better when compared with other summarisers after evaluation using ROUGE metrics."
An Approach for Query-Focused Text Summarisation for Evidence Based Medicine,"We present an approach for extractive, query-focused, single-document summarisation of medical text. Our approach utilises a combination of target-sentence-specific and target-sentence-independent statistics derived from a corpus specialised for summarisation in the medical domain. We incorporate domain knowledge via the application of multiple domain-specific features, and we customise the answer extraction process for different question types. The use of carefully selected domain-specific features enables our summariser to generate content-rich extractive summaries, and an automatic evaluation of our system reveals that it outperforms other baseline and benchmark summarisation systems with a percentile rank of 96.8%."
A Novel Approach to Text Summarisation Using Topic Modelling and Noun Phrase Extraction,"Over the past few years, one of the remarkable developments that happened on the web is the rapid growth of textual data. This substantial increase, however, induces a complication in the retrieval of vital information from the digitized collection of data. The conventional technique used to tackle this problem is Automatic Text Summarisation. This technique extracts the essential words or sentences from the data and summarises it without affecting the semantics. Automatic text summarisation is classified into two, Extractive and Abstractive. The Extractive method summarises a document by selecting the important words or sentences from it, based on some attributes while the Abstractive method attempts to generate its summary from the semantics of the data. In this paper, we propose a novel approach in Extractive text summarisation by using a new sentence scoring parameter. The experimental results show that the proposed sentence scoring parameter improves the performance of the Extractive text summariser, when compared with other summarisation models. To validate our proposed model, we compared it with four commonly used summarisation models on grounds of ROUGE-1 score and F1 score."
A method of automatic text summarisation based on long short-term memory,"Deep learning is currently developing very fast in the NLP field and has achieved many amazing results in the past few years. Automatic text summarisation means that the abstract of the document is automatically summarised by a computer program without changing the original intention of the document. There are many application scenarios for automatic summarisation, such as news headline generation, scientific document abstract generation, search result segment generation, and product review summarisation. In the era of internet big data in the information explosion, if the short text can be employed to express the main connotation of information, it will undoubtedly help to alleviate the problem of information overload. In this paper, a model based on the long short-term memory network is presented to automatically analyse and summarise Chinese articles by using the seq2seq+attention models. Finally, the experimental results are attached and evaluated."
Software solution for text summarisation using machine learning based Bidirectional Encoder Representations from Transformers algorithm,"The data science has evolved over the past 2 decades, allowing the technical norms to be built in a way that can handle the new issues. While various technical issues develop, the requirement for text summary has always been there. Nearly 10 years ago, the foundation of automatic text summarisation was created, and since then, technical improvements and refinements have been made for large-scale big data handling, crime investigation, and cybersecurity, to name a few. There are several text summarising techniques, and these influence the outcomes as well. Another difference from the last 20 years is the requirement for time for text summarising. To pursue acquiring the findings, machine learning methods are applied to the core set of text phrases as a data set. Neural networks are now being used to improve text summarisation. But because there is more data available as short text to summarise, there will also be a big need for short text summarising. Utilising a quick summarising technique, accuracy, precision, and memory are improved. The focus of the challenge in this work is on brief text summarisation and boosting accuracy using a cutting-edge algorithm called Bidirectional Encoder Representations from Transformers (BERT). Bidirectional Encoder Representations from Transformers with transformer produced outstanding results for Short Text Summarisation. The model receives the input and performs a sequence-to-sequence analysis of the data down to the word level. The model's implementation is then contrasted with Word2Vec + RNN and Word2Vec + long short-term memory (LSTM), two earlier works. The proposed strategy, which seeks to increase the training data duration and accuracy for short text summarisation, produced best results utilising BERT + LSTM and BERT + Transformer. Using a confusion matrix to monitor and analyse the improved findings, it was shown that BERT + Transformer had an accuracy of 97%. The suggested model also performs better than existing models in terms of precision (46%), and recall (30%)."
Testing Methodologies and Exploring Challenges and Issues in Text Summarisation,"Now in a day millions of products and customers are adding to the market. In current era, a wide range of customers are putting their interest to purchase products from the internet. So it becomes necessary that quality products can be purchased by customer with greater satisfaction. In recommender systems, items are recommended to users based on their search query, as well as their context information including history of purchased items, taste, location, gender and age group of the user and the sentiment about the products of inventory by analysing product reviews. We can solve this problem by considering cooperative environment, increase the performance of the learners compared to the best recommendation method that comprises the complete realisation of user arrivals and the inventory of items, as well as the context-dependent effects on purchase of items, and evaluate results on a dataset. The main idea behind to choosing sentiment analysis to recommend items according the popularity of the items and review of other users about the same items to convince user needs as per elicited user preferences expressed in textual reviews because now in a day's wide range of users of social networking sites passing their sentiments in reviews about different products. The review datasets consist of millions of reviews which plays an important role to improve the prediction accuracy. This technique is named as sentiment analysis and maps such preferences onto some predicted rating scales that can be understood by existing recommendation algorithms mainly done by extracting opinion words from review and then aggregating the ratings of such words to determine the dominant or average sentiment implied by the user for particular item.
"
The impact of summarisation on textual entailment - a case study on global warming arguments,"The challenging task of recognizing textual entailment aims to check whether the meaning of a smaller text - the hypothesis h - can be inferred from another text T. Current methods interleave natural language processing, machine learning, search and lexical resources. All these instruments pose computational challenges that make textual entailment unfeasible for large texts. Hence, we investigate how textual entailment is affected by text summarization. By summarising the text T we expect a decrease of accuracy, but an increase of computation speed. We aim to assess the expected decrease in accuracy caused by summarisation against time benefits due to smaller text given to entailment machinery. Our results show that the time needed for computing entailment is decreased four times, while the accuracy decreases with two percentages."
Diffusion of Abstractive Summarisation to Improve Ease of Use and Usefulness,"Even though the abstractive summarisation method has been in existence for over four decades, its popularity, adoption and diffusion have been slow, or limited, in both business and academic domains. This has retarded advances of the innovation and hampered its potential benefits to users. Thus, this qualitative, empirical study was undertaken to examine and understand how the abstractive summarisation method can be diffused to increase its spread and usefulness across business and academic environments. The 1995 theory of Diffusion of Innovation (DOI) by Rogers was employed in the analysis of the existing works. Based on analysis, five main factors were identified as influencing the adoption of the abstractive summarisation method. The factors are complexity, practical effectiveness, experimental, shortened output and lack of friendliness. These factors were discussed towards gaining a better understanding of how the diffusion of abstractive summarisation method can be improved in both business and academic environments."
Summarising company announcements,"This paper describes work that attempts to use language technology as a solution to the problem of information overload. The specific domain of application is the database of company announcements accessible via the Web site of the Australian Stock Exchange to meet regulatory requirements, over 100,000 documents a year are made available via this site, with only limited search facilities. We use a variety of techniques from language technology to make it easier to explore and manage the information in this data set. In this paper, we focus on our use of information extraction, which identifies and extracts important elements of information from a document, and text compaction, which applies linguistically-motived substitutions to reduce potential summary sentences to more compact forms. Together, these techniques provide a way of producing summaries of a significant proportion of the document base."
Fuzzy logic-based single document summarisation with improved sentence scoring technique,"Text summarisation is compressed or condensed version of any text document. Due to increasing use of digitisation, massive amount of information is available on internet. Text summarisation is an emerging alternative for users to find relative information in automated shortened versions. In this paper we propose single document summarisation technique using shallow features of sentence to generate summary. The weight of sentences is calculated by applying score of different words and sentence-based statistical features. Here, most salient sentences are selected based on weight of sentences and are put together to generate summary. This is modeled using fuzzy inference system. This approach utilises fuzzy inference and fuzzy measures to find most significant sentences. The result of our proposed method is compared with other methods using recall oriented understudy for Gisting evaluation (ROUGE-N) measures on document understanding conferences (DUC) 2002 dataset and results show that our proposed method outperforms a few baseline methods."
A Semi-Automated Approach to Building Text Summarisation Classifiers,"An investigation into the extraction of useful information from the free text element of questionnaires, using a semi-automated summarisation extraction technique to generate text summarisation classifiers, is described. A realisation of the proposed technique, SARSET (Semi-Automated Rule Summarisation Extraction Tool), is presented and evaluated using real questionnaire data. The results of this approach are compared against the results obtained using two alternative techniques to build text summarisation classifiers. The first of these uses standard rule-based classifier generators, and the second is founded on the concept of building classifiers using secondary data. The results demonstrate that the proposed semi-automated approach outperforms the other two approaches considered."
